### snakemake workflow to simulated data, run our pipeline on it, and then analyse the results

import sys
import os
import yaml
import pdb
import copy
import itertools
import pandas as pd
import re

from python_scripts.parse_config import parse_config
from python_scripts.make_df import make_df, make_post_args, make_reference_dict

from python_scripts.make_df import check_input_files, Error, InputError, check_dataset_sample_unique, make_reference_dict, make_post_args, check_fastas_unique


# https://sapac.support.illumina.com/bulletins/2016/12/what-sequences-do-i-use-for-adapter-trimming.html
HS25 = {'read1-adapt' : "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA", 
				'read2-adapt' : "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"}
				

adapter_seqs = {'HS25':HS25, 'MSv3':HS25}

#####################################################
############# simulation parameters #################
#####################################################


# make dataframe with parameters for simulation
config, sim_df, ref_dict = parse_config(config)


#####################################################
############## analysis parameters ##################
#####################################################


# get unique analysis conditions - these are combinations of the analysis parameters that
# can be set in our pipeline (merging, de-duplicaiton, bwa mem prarameters, etc), or
# the tool to be used in analysis

analysis_conditions = []
datasets = list(config.keys())

column_names = ('experiment', 'exp', 'analysis_condition', 'tool', 'host', 'host_fasta',
								'virus', 'virus_fasta', 'bam_suffix',
								'read_folder', 'R1_suffix', 'R2_suffix', 'outdir', 
								'merge', 'dedup', 'postargs', 'seq_sys', 'adapter_1', 'adapter_2', 
								'bwa_mem_params', 'score_ints', 'score_ints_window', 'score_ints_tool')

for dataset in datasets:
	
		#### parameters for comparison of analysis and simulation ####	
	# are we doing per-integration comparison of simulation with results?
	score_ints= 0
	score_ints_window = None
	if 'score_ints' in config[dataset]:
		if config[dataset]['score_ints'] is True or config[dataset]['score_ints'].lower() == 'true':
			score_ints = 1
			# check if we've specified a window around simulated integrations to look
			if 'score_ints_window' in config[dataset]:
				score_ints_window = config[dataset]['score_ints_window']
	
	#### parameters for our pipeline ####
	# paramters for our pipeline should be lists, so that we can do combinations
	# of each 
	assert hasattr(config[dataset]['merge'], '__iter__')
	assert hasattr(config[dataset]['dedup'], '__iter__')
	assert hasattr(config[dataset]['merge'], '__iter__')
	assert hasattr(config[dataset]['post'], '__iter__')
	assert hasattr(config[dataset]['bwa_mem'], '__iter__')	
	assert hasattr(config[dataset]['seq_sys'], '__iter__')
	
	# each combination of these are a unique 'analysis condition' for our pipeline
	i = 0
	for merge, dedup, post, bwa_mem_params, seq_sys, host, virus in itertools.product(
																		config[dataset]['merge'], 
																		config[dataset]['dedup'],
																		config[dataset]['post'],
																		config[dataset]['bwa_mem'],
																		config[dataset]['seq_sys'],
																		config[dataset]['analysis_hosts'].keys(),
																		config[dataset]['analysis_viruses'].keys()):
		
		condition = f"{dataset}_analysis{i}"

		# format parameters required for this condition
		post_args = make_post_args({dataset: {'post':post}})[0][dataset]
		
		# figure out which adapters to use, based on which sequencing system used for art
		read1_adapt = adapter_seqs[seq_sys]['read1-adapt']
		read2_adapt = adapter_seqs[seq_sys]['read2-adapt']
		
		# convert merge and dedup to 1 or 0
		if merge is True or merge == 'true' or merge == 'True':
			merge = 1
		else:
			merge = 0
			
		if dedup is True or dedup == 'true' or dedup == 'True':
			dedup = 1
		else:
			dedup = 0

		analysis_conditions.append({
			'experiment' : dataset,
			'host' 			: host,
			'host_fasta': config[dataset]['analysis_hosts'][host],
			'virus'     : virus,
			'virus_fasta': config[dataset]['analysis_viruses'][virus],
			'analysis_condition': condition,
			'merge'			 : merge,
			'dedup'			 : dedup,
			'postargs'  : make_post_args({dataset: {'post':post}})[0][dataset],
			'adapter_1'  : read1_adapt,
			'adapter_2'  : read2_adapt,
			'bwa_mem_params': bwa_mem_params,
			'tool'			 : 'pipeline',
			'score_ints' : score_ints,
			'score_ints_window' : score_ints_window,
			'score_ints_tool' : "pipeline"	
		})
		i += 1
	
	#### parameters for polyidus ####
	i = 0
	if 'polyidus_params' in config[dataset]:
			
			# are we trying multiple aligners?
			if 'aligner' in config[dataset]['polyidus_params']:
				assert hasattr(config[dataset]['polyidus_params']['aligner'], '__iter__')
				aligners = config[dataset]['polyidus_params']['aligner']
			else:
				aligners = ['bowtie2']
				
			for host, virus, aligner in itertools.product(
																		config[dataset]['analysis_hosts'].keys(),
																		config[dataset]['analysis_viruses'].keys(),
																		aligners):
				# give this analysis condition a name
				condition = f"{dataset}_polyidus{i}"

				analysis_conditions.append({
					'experiment' : dataset,
					'host' 			 : host,
					'host_fasta' : config[dataset]['analysis_hosts'][host],
					'virus'      : virus,
					'virus_fasta': config[dataset]['analysis_viruses'][virus],
					'analysis_condition': condition,
					'aligner'		 : aligner,
					'tool'			 : 'polyidus',
					'score_ints' : score_ints,
					'score_ints_window' : score_ints_window,
					'score_ints_tool' : "polyidus"	
					})	
				i += 1
	
	#### parameters for vifi ####
	# if we're also doing vifi
	i = 0
	if 'vifi' in config[dataset]:
		if config[dataset]['vifi'] is True or config[dataset]['vifi'] == 'true' or config[dataset]['vifi'] == 'True':
			for host, virus in itertools.product(
																config[dataset]['analysis_hosts'].keys(),
																config[dataset]['analysis_viruses'].keys()):
				condition = f"{dataset}_vifi{i}"
				analysis_conditions.append({
					'experiment' : dataset,
					'host' 			 : host,
					'host_fasta' : config[dataset]['analysis_hosts'][host],
					'virus'      : virus,		
					'virus_fasta': config[dataset]['analysis_viruses'][virus],	
					'analysis_condition': condition,
					'tool'			 : 'vifi',			
					'score_ints' : score_ints,
					'score_ints_window' : score_ints_window,
					'score_ints_tool' : "vifi"	
				})
				i += 1

##### parameters that are common to each experiment ####
for cond_dict in analysis_conditions:
	dataset = cond_dict['experiment']
	cond_dict['bam_suffix'] = '.sam'
	cond_dict['R1_suffix'] = '1.fq'
	cond_dict['R2_suffix'] = '2.fq'
	cond_dict['read_folder'] = os.path.join(config[dataset]['out_directory'], dataset, "sim_reads")
	cond_dict['outdir'] = config[dataset]['out_directory'] 
	cond_dict['score_ints'] = score_ints
	cond_dict['score_ints_window'] = score_ints_window
	cond_dict['exp'] = cond_dict['experiment']

	
# make data frame 
analysis_df = pd.DataFrame(analysis_conditions, columns = column_names)



## our pipeline also requires a dataframe called 'toDo', containing the analysis paramters for each sample in each dataset.  Build this using sim_df (from simulation) and analysis_df (analysis parameters).  This must have the following columns:

column_names = ['dataset', 'config_dataset', 'experiment', 'sample', 'host', 'host_fasta', 'virus', 'virus_fasta', 'merge', 'dedup', 'unique', 'outdir', 'bwa_mem_params', 'R1_file', 'R2_file', 'bam_file', 'adapter_1', 'adapter_2', 'postargs']

rows = []

# build rows of toDo
for i, row in analysis_df.iterrows():

	# skip any rows we don't want to analyse with our pipeline
	if row['tool'] != 'pipeline':
		continue

	# get samples for this simulated dataset
	experiment = row['experiment']
	dataset = row['analysis_condition']
	samples = sim_df[sim_df['experiment'] == experiment]['sample']
	
	# make a row for each sample in this dataset
	for sample in samples:
		
		# use information we already worked out from config
		todo_row = dict(row)
		todo_row['dataset'] = dataset
		todo_row['config_dataset'] = experiment
		
		
		# add sample name
		todo_row['sample'] = sample
		todo_row['unique'] = f"{dataset}+++{sample}"
		
		# read file names
		read_prefix = os.path.join(row['outdir'], experiment, 'sim_reads', sample)
		todo_row['R1_file'] = read_prefix + row['R1_suffix']
		todo_row['R2_file'] = read_prefix + row['R2_suffix']
		todo_row['bam_file'] = read_prefix + row['bam_suffix']
		
		rows.append(todo_row)
		
toDo = pd.DataFrame(rows, columns = column_names)

# make dictionary of reference names and fasta files
ref_names = make_reference_dict(toDo)

# construct arguments for postprocess.R script for each dataset

POSTARGS, TOSORT, SORTED = make_post_args(config)


#####################################################
############ wildcard constraints ###################
#####################################################


wildcard_constraints:
	cond = "|".join(set(sim_df.loc[:, 'condition'])),
	exp = "|".join(set(sim_df.loc[:, 'experiment'])),
	rep = "|".join(set([str(i) for i in sim_df.loc[:, 'replicate']])),
	dset = "|".join(set(analysis_df.loc[:, 'analysis_condition'])),
	samp = "|".join(set(toDo.loc[:, 'sample'])),
	host = "|".join(set(analysis_df.loc[:, 'host'])),
	virus = "|".join(set(analysis_df.loc[:, 'virus'])),
	post = "|\.post",
	analysis_condition = "|".join(analysis_df['analysis_condition']),
	outpath = "|".join(set(analysis_df.loc[:, 'outdir'])),
	
localrules: write_summary, write_analysis_summary#, combine_int_scores, combine_read_scores


#####################################################
################## ruleorder ########################
#####################################################

# to resolve conflicts between polyidus and combine_ints (our analysis pipeline)
# rule polyidus has a wildcard contstraint so it can't be used for our pipeline
# so make it higher in the priority

ruleorder: polyidus > combine_ints

#####################################################
################### target files ####################
#####################################################

# target files for simulation
sim_targets = sim_df.loc[:,'annotated_info_filename']

exp_targets = {}
for i, row in sim_df.iterrows():
	exp_targets[row['experiment']] = row['out_directory']
experiments = list(exp_targets.keys())

sim_summaries = expand("{outpath}/{exp}/simulation_summary.tsv", 
			zip,
			exp = experiments, 
			outpath = [exp_targets[exp] for exp in experiments]
			)

			
analysis_summaries = [f"{outdir}/{experiment}/analysis_conditions.tsv" for outdir, experiment in 
												zip(sim_df.loc[:,'out_directory'],
														sim_df.loc[:,'experiment']
														)
											]

# polyidus outputs
polyidus_targets = []
for i, row in analysis_df.iterrows():
	if row['tool'] != 'polyidus':
		continue
		
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['analysis_condition']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
	host = row['host']
	virus = row['virus']
		
	for samp in samples:
		polyidus_targets.append(
			f"{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.txt"
		)
		polyidus_targets.append(
			f"{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}.tsv"
		)

	
# summary files

# get experiments for which we want to score integrations
exps_int_score = [exp for exp in config if 'score_ints' in config[exp]]
exps_int_score = [exp for exp in exps_int_score if config[exp]['score_ints']]

scored_summaries = expand('{outpath}/{exp}/{exp}.scored_reads_summary.tsv', 
				zip,
				exp = config.keys(), 
				outpath = [exp_targets[exp] for exp in config]
			) + expand(
			"{outpath}/{exp}/{exp}.scored_ints_summary.tsv",
				zip,
				exp = exps_int_score, 
				outpath = [exp_targets[exp] for exp in exps_int_score]
			) 
			


rule all:
	input: 
		set(sim_targets),
		set(sim_summaries),
		set(analysis_summaries),
		#set(post_targets),
		set(scored_summaries),
		set(polyidus_targets)
		
		

#####################################################
############### simulate integrations ###############
#####################################################


include: "snakemake_rules/simulate_integrations.smk"
include: "snakemake_rules/art.smk"
include: "snakemake_rules/annotate_reads.smk"

#####################################################
################### analyse data ####################
#####################################################

include: "snakemake_rules/preprocessing.smk"
include: "snakemake_rules/alignment.smk"
include: "snakemake_rules/find_ints.smk"
include: "snakemake_rules/postprocessing.smk"


#####################################################
##################### polyidus ######################
#####################################################
def analysis_df_value(wildcards, column_name):
	
	# get a value from the row of the df corresponding to this analysis condition
	unique = f"{wildcards.dset}"

	return analysis_df.loc[(analysis_df['analysis_condition'] == unique).idxmax(), column_name] 
	
def sim_df_value(wildcards, column_name):
	
	
	# get a value from the row of the df corresponding to this analysis condition
	unique = f"{wildcards.exp}__{wildcards.samp}"
	return sim_df.loc[(sim_df['unique'] == unique).idxmax(), column_name] 
	

rule bwt2_index:
	input:
		fasta = lambda wildcards: ref_names[wildcards.genome]
	output:
		"{outpath}/references/{genome}/{genome}.1.bt2"
	container:
		"docker://szsctt/polyidus:2"
	params:
		prefix = lambda wildcards, output: path.splitext(path.splitext(output[0])[0])[0]
	resources:
		mem_mb= lambda wildcards, attempt, input: int(attempt * 5 * (os.stat(input.fasta).st_size/1e6)),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"bowtie2-build {input} {params.prefix}"

def get_polyidus_reads(wildcards, read_num):
	assert read_num in (1, 2)
	if read_num == 1:
		return f"{wildcards.outpath}/{analysis_df_value(wildcards, 'exp')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, 'R1_suffix')}"
	if read_num == 2:
		return f"{wildcards.outpath}/{analysis_df_value(wildcards, 'exp')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, 'R2_suffix')}"	

rule polyidus:
	input:
		fastq1 = lambda wildcards: get_polyidus_reads(wildcards, 1),
		fastq2 = lambda wildcards: get_polyidus_reads(wildcards, 2),
		host_idx =  "{outpath}/references/{host}/{host}.1.bt2",
		virus_idx = "{outpath}/references/{virus}/{virus}.1.bt2"
	output:
		temp_ints = temp("{outpath}/{dset}/polyidus_{host}_{virus}_{samp}/results/exactHpvIntegrations.tsv"),
		ints = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.txt"
	params:
		output = lambda wildcards, output: path.dirname(path.dirname(output.temp_ints)),
		host_idx = lambda wildcards, input: path.splitext(path.splitext(input.host_idx)[0])[0],
		virus_idx = lambda wildcards, input: path.splitext(path.splitext(input.virus_idx)[0])[0],
	resources:
		mem_mb= lambda wildcards, attempt: int(attempt * 10000),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1]
	container:
		"docker://szsctt/polyidus:2"
	wildcard_constraints:
		dset = ".+_polyidus\d+"
	shell:
		"""
		rm -r {params.output}/*
		
		python3 /usr/src/app/src/polyidus.py \
		{params.host_idx} \
		{params.virus_idx} \
		--fastq {input.fastq1} {input.fastq2} \
		--outdir {params.output}
		
		cp {output.temp_ints} {output.ints}
		"""

#####################################################
####################### vifi ########################
#####################################################

rule make_data_repo:
	input:
		lambda wildcards: ref_names[wildcards.genome]
	output:
		"{outpath}/vifi_refs/data_repo/{host}/{host}.1.bt2"
	container:
		"docker://szsctt/polyidus:1"
	params:
		prefix = lambda wildcards, output: path.splitext(path.splitext(output[0])[0])[0]
	shell:
		"bowtie2-build {input} {params.prefix}"
		
		

#####################################################
############ compare sim with analysis ##############
#####################################################


rule write_analysis_summary:
	output:
		pipeline_conditions = "{outdir}/{experiment}/pipeline_analysis_conditions.tsv",
		analysis_conditions = "{outdir}/{experiment}/analysis_conditions.tsv"
	run:
		toDo.to_csv(output.pipeline_conditions, sep='\t', index=False)
		analysis_df.to_csv(output.analysis_conditions, sep='\t', index=False)

		
rule score_reads:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		sim_sam = "{outpath}/{exp}/sim_reads/{samp}.sam",
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt"
	output:
		scored_reads = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}.tsv",
		summary = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}_summary.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 50000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	threads: 10
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_reads.py \
		--sim-info {input.sim_info} \
		--sim-sam {input.sim_sam} \
		--analysis-info {input.analysis_info} \
		--output {output.scored_reads} \
		--output-summary {output.summary} \
		--threads {threads}
		"""


rule combine_read_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "reads")
	output:
		"{outpath}/{exp}/{exp}.scored_reads_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""



rule score_integrations:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt"
	output:
		scored_ints = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.tsv",
		summary = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}_summary.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	params:
		tool = lambda wildcards: f"--analysis-tool {analysis_df_value(wildcards, 'score_ints_tool')}",
		window = lambda wildcards: f"--window {analysis_df_value(wildcards, 'score_ints_window')}"
	threads: 1
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_integrations.py \
		--sim-info {input.sim_info} \
		--found-info {input.analysis_info} \
		--output {output.scored_ints} \
		--summary {output.summary} \
		{params}
		"""
		
rule combine_int_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "ints")
	output:
		"{outpath}/{exp}/{exp}.scored_ints_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""
		
# target files for per-read or per-integration comparison of analysis with simulation
def scores_to_combine(wildcards, score_type):

	assert score_type in ("ints", "reads")
	
	if score_type == "ints":
		folder = "scored_ints"
	else:
		folder = "scored_reads"
	
	scored_files = []
	for i, row in analysis_df.iterrows():
		
		# get information for this row
		exp = row['experiment']

		if exp !=  wildcards.exp:
			continue
		outpath = row['outdir']
		analysis_condition = row['analysis_condition']
		host = row['host']
		virus = row['virus']
	
	
		# get samples for this experiment
		samples = set(sim_df[sim_df['experiment'] == exp]['sample'])
	
		# we only have postprocessed data for our pipeline
		if re.search("analysis", analysis_condition):
			
			post = ['.post', '']

		else:
			post = ['']
			
			# currently we're only scoring reads for our pipeline
			if score_type == "reads":
				continue
	
		scored_files += [
		f"{outpath}/{exp}/{folder}/{analysis_condition}.{samp}.{host}.{virus}{post}_summary.tsv" 
			for samp, post,
			in itertools.product(samples, post)
		]
	
	return scored_files

