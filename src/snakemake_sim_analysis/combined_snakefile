### snakemake workflow to simulated data, run our pipeline on it, and then analyse the results

import sys
import os
import yaml
import pdb
import copy
import itertools
import pandas as pd
import re

from python_scripts.parse_config import parse_config
from python_scripts.make_df import make_df, make_post_args, make_reference_dict

from python_scripts.make_df import check_dataset_sample_unique, make_reference_dict, make_post_args, check_fastas_unique

from scripts.parse_analysis_config import parse_analysis_config, get_bool_value_from_config


# https://sapac.support.illumina.com/bulletins/2016/12/what-sequences-do-i-use-for-adapter-trimming.html
HS25 = {'read1-adapt' : "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA", 
				'read2-adapt' : "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"}		

adapter_seqs = {'HS25':HS25, 'MSv3':HS25, 'MS': HS25}

verse_threads = 8

#defaults for scoring
default_scoring = {'score_ints' : 0, 'score_merged_ints' : 1, 'score_reads' : 0}

#####################################################
############### global parameters ###################
#####################################################

# global parameters (defaults for this dataset) may be specified using a 'global' key
# at the top of the config file.  These will be applied to any missing keys in the remaining 
# datasets in the config file.  If a key is specified in both 'global' and a dataset,
# the value specified in the dataset will be used

if 'global' in config:		
	# get default (global) options
	default = config.pop('global')
	for dataset in config:
		for key in default:
			if key not in config[dataset]:
				config[dataset][key] = default[key]
			


#####################################################
############# simulation parameters #################
#####################################################


# make dataframe with parameters for simulation
config, sim_df, ref_dict = parse_config(config)


#####################################################
############## analysis parameters ##################
#####################################################

def convert_bool_to_int(input_bool, dataset, name):
	if isinstance(input_bool, bool):
		if input_bool:
			return 1
		else:
			return 0
	elif isinstance(input_bool, str):
		if input_bool.lower() == "true":
			return 1
		else:
			return 0
	else:
		raise ValueError(f"Unrecgonised input for key {name} in dataset {dataset}")


# make data frame for other tools
analysis_df = parse_analysis_config(config)

# assume that we're doing our pipeline on all datasets
extra_rows = []							
for dataset in config:
	
	#### parameters for our pipeline ####
	# paramters for our pipeline should be lists, so that we can do combinations
	# of each 
	assert hasattr(config[dataset]['merge'], '__iter__')
	assert hasattr(config[dataset]['dedup'], '__iter__')
	assert hasattr(config[dataset]['trim'], '__iter__')
	assert hasattr(config[dataset]['post'], '__iter__')
	assert hasattr(config[dataset]['bwa_mem'], '__iter__')	
	assert hasattr(config[dataset]['seq_sys'], '__iter__')
	assert hasattr(config[dataset]['merge-dist'], '__iter__')
	assert hasattr(config[dataset]['merge-n-min'], '__iter__')
	assert hasattr(config[dataset]['clip-cutoff'], '__iter__')
	assert hasattr(config[dataset]['cigar-tol'], '__iter__')
	assert hasattr(config[dataset]['min-mapq'], '__iter__')
	
	scoring = {}
	for score_type, default in default_scoring.items():
		scoring[score_type] = get_bool_value_from_config(config, dataset, score_type, default)
	if scoring['score_ints'] == 1 or scoring['score_merged_ints'] == 1:
		score_ints_window = config[dataset]['score_ints_window']
	else:
		score_ints_window = 0

	# each combination of these are a unique 'analysis condition' for our pipeline
	i = 0
	for merge, trim, dedup, post, bwa_mem_params, seq_sys, host, virus, merge_dist, merge_n_min, clip_cutoff, cigar_tol, min_mapq in itertools.product(
																		config[dataset]['merge'], 
																		config[dataset]['trim'], 
																		config[dataset]['dedup'],
																		config[dataset]['post'],
																		config[dataset]['bwa_mem'],
																		config[dataset]['seq_sys'],
																		config[dataset]['analysis_hosts'].keys(),
																		config[dataset]['analysis_viruses'].keys(),
																		config[dataset]['merge-dist'],
																		config[dataset]['merge-n-min'],
																		config[dataset]['clip-cutoff'],
																		config[dataset]['cigar-tol'],
																		config[dataset]['min-mapq'],
																		):
		
		condition = f"{dataset}_analysis{i}"

		# format parameters required for this condition
		post_args = make_post_args({dataset: {'post':post}})[0][dataset]
		
		# figure out which adapters to use, based on which sequencing system used for art
		read1_adapt = adapter_seqs[seq_sys]['read1-adapt']
		read2_adapt = adapter_seqs[seq_sys]['read2-adapt']
		
		# convert merge and dedup to 1 or 0			
		merge = convert_bool_to_int(merge, dataset, 'merge')

		dedup = convert_bool_to_int(dedup, dataset, 'dedup')
		
		trim = convert_bool_to_int(trim, dataset, 'trim')
			
		extra_rows.append({
			'experiment' : dataset,
			'host' 			: host,
			'host_fasta': config[dataset]['analysis_hosts'][host],
			'virus'     : virus,
			'virus_fasta': config[dataset]['analysis_viruses'][virus],
			'analysis_condition': condition,
			'merge'			 : merge,
			'trim'			 : trim,
			'dedup'			 : dedup,
			'postargs'  : make_post_args({dataset: {'post':post}})[0][dataset],
			'adapter_1'  : read1_adapt,
			'adapter_2'  : read2_adapt,
			'bwa_mem_params': bwa_mem_params,
			'clip_cutoff': clip_cutoff,
			'cigar_tol'  : cigar_tol,
			'min_mapq'  : min_mapq,
			'tool'			 : 'pipeline',
			'merge_dist' : merge_dist,
			'merge_n_min': merge_n_min,
			'score_ints_window' : score_ints_window,
			**scoring
		})
		i += 1

	
extra = pd.DataFrame(extra_rows)
analysis_df = pd.concat((extra, analysis_df), axis=0, ignore_index=True)

# 	##### parameters that are common to each experiment ####

analysis_df['bam_suffix'] = ['.sam' for i in range(analysis_df.shape[0])]
analysis_df['R1_suffix'] = ['1.fq' for i in range(analysis_df.shape[0])]
analysis_df['R2_suffix'] = ['2.fq' for i in range(analysis_df.shape[0])]

analysis_df['read_folder'] = [os.path.join(config[dataset]['out_directory'], dataset, "sim_reads") for dataset in analysis_df['experiment']]

analysis_df['outdir'] = [config[dataset]['out_directory'] for dataset in analysis_df['experiment']]
analysis_df['exp'] = analysis_df['experiment']	

# also add columns indicating if we want to score integrations, merged integrations, and reads

# rows to add 'score_reads' 'score_ints', 'score_merged_ints', 'score_ints_window',  for other tools
# we only score reads from our pipeline

score_reads = []
score_ints = []
score_merged_ints = []
score_ints_window = []
for i, row in analysis_df.iterrows():
	# only score reads from our pipeline
	if row['tool'] != 'pipeline':
		score_reads.append(0)
	else:
		score_reads.append(get_bool_value_from_config(config, row['experiment'], 'score_reads', default_scoring['score_reads']))
	
	# scoring ints, merged_ints
	score_ints.append(get_bool_value_from_config(config, row['experiment'], 
																							'score_ints', default_scoring['score_ints']))
	score_merged_ints.append(get_bool_value_from_config(config, row['experiment'], 
																							'score_merged_ints', default_scoring['score_merged_ints']))
																							
	# get score_ints_window if appropriate
	if score_ints[i] == 1 or score_merged_ints[i] == 1:
		score_ints_window.append(config[row['experiment']]['score_ints_window'])
	else:
		score_ints_window.append(None)
		
analysis_df['score_reads'] = score_reads
analysis_df['score_ints'] = score_ints
analysis_df['score_merged_ints'] = score_merged_ints
analysis_df['score_ints_window'] = score_ints_window

## our pipeline also requires a dataframe called 'toDo', containing the analysis paramters for each sample in each dataset.  Build this using sim_df (from simulation) and analysis_df (analysis parameters).  This must have the following columns:

column_names = ['dataset', 'config_dataset', 'experiment', 'sample', 'host', 'host_fasta', 'virus', 'virus_fasta', 'merge', 'trim', 'dedup', 'unique', 'outdir', 'bwa_mem_params', 'R1_file', 'R2_file', 'bam_file', 'adapter_1', 'adapter_2', 'postargs', 'merge_dist', 'merge_n_min', 'clip_cutoff', 'cigar_tol', 'min_mapq']

rows = []

# build rows of toDo
for i, row in analysis_df.iterrows():

	# skip any rows we don't want to analyse with our pipeline
	if row['tool'] != 'pipeline':
		continue

	# get samples for this simulated dataset
	experiment = row['experiment']
	dataset = row['analysis_condition']
	samples = sim_df[sim_df['experiment'] == experiment]['sample']
	
	# make a row for each sample in this dataset
	for sample in samples:
		
		# use information we already worked out from config
		todo_row = dict(row)
		todo_row['dataset'] = dataset
		todo_row['config_dataset'] = experiment
		
		# add sample name
		todo_row['sample'] = sample
		todo_row['unique'] = f"{dataset}+++{sample}"
		
		# read file names
		read_prefix = os.path.join(row['outdir'], experiment, 'sim_reads', sample)
		todo_row['R1_file'] = read_prefix + row['R1_suffix']
		todo_row['R2_file'] = read_prefix + row['R2_suffix']
		todo_row['bam_file'] = read_prefix + row['bam_suffix']
		
		rows.append(todo_row)
		
toDo = pd.DataFrame(rows, columns = column_names)

# make dictionary of reference names and fasta files
ref_names = make_reference_dict(toDo)

# construct arguments for postprocess.R script for each dataset

POSTARGS, TOSORT, SORTED = make_post_args(config)

#####################################################
############ wildcard constraints ###################
#####################################################

wildcard_constraints:
	cond = "|".join(set(sim_df.loc[:, 'condition'])),
	exp = "|".join(set(sim_df.loc[:, 'experiment'])),
	rep = "|".join(set([str(i) for i in sim_df.loc[:, 'replicate']])),
	dset = "|".join(set(analysis_df.loc[:, 'analysis_condition'])),
	samp = "|".join(set(toDo.loc[:, 'sample'])),
	host = "|".join(set(analysis_df.loc[:, 'host'])),
	virus = "|".join(set(analysis_df.loc[:, 'virus'])),
	post = "|\.post",
	merged = "|\.merged",
	analysis_condition = "|".join(analysis_df['analysis_condition']),
	outpath = "|".join(set(analysis_df.loc[:, 'outdir'])),
	
localrules: write_summary, write_analysis_summary, write_analysis_summary_all_tools#, combine_int_scores, combine_read_scores

#####################################################
################## ruleorder ########################
#####################################################

# to resolve conflicts between polyidus and combine_ints (our analysis pipeline)
# rule polyidus has a wildcard contstraint so it can't be used for our pipeline
# so make it higher in the priority

ruleorder: vifi > polyidus > verse > seeksv > combine_ints > merge_host_bed > merged_bed > trim > seqPrep_unmerged

#####################################################
################### target files ####################
#####################################################

# target files for simulation
sim_targets = sim_df.loc[:,'annotated_info_filename']

# summaries about how experiment was conducted
exp_targets = {}
for i, row in sim_df.iterrows():
	exp_targets[row['experiment']] = row['out_directory']
experiments = list(exp_targets.keys())

sim_summaries = expand("{outpath}/{exp}/simulation_summary.tsv", 
			zip,
			exp = experiments, 
			outpath = [exp_targets[exp] for exp in experiments]
			)

analysis_summaries = expand("{outpath}/{exp}/analysis_conditions.tsv", 
			zip,
			exp = experiments, 
			outpath = [exp_targets[exp] for exp in experiments]
			)

# outputs from our pipeline
merged_targets = set()
for i, row in analysis_df.iterrows():
	if row['tool'] != 'pipeline':
		continue
		
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['analysis_condition']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
	host = row['host']
	virus = row['virus']
		
	for samp, post in itertools.product(samples, (".post", "")):
		merged_targets.add(
			f"{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.merged.bed"
		)

# targets for other tools
other_tool_targets = set()
for i, row in analysis_df.iterrows():
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['analysis_condition']
	host = row['host']
	virus = row['virus']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
		
	for samp in samples:
		other_tool_targets.add(
			f"{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.txt"
		)

# scored reads
scored_summaries = set()
for i, row in analysis_df.iterrows():
	if row['score_ints'] == 1 or row['score_merged_ints'] == 1:
		scored_summaries.add(f"{row['outdir']}/{row['experiment']}/{row['experiment']}.scored_ints_summary.tsv")
	if 'score_reads' in row:
		if row['score_reads'] == 1:
			scored_summaries.add(f"{row['outdir']}/{row['experiment']}/{row['experiment']}.scored_reads_summary.tsv")

rule all:
	input: 
		set(sim_targets),
		set(sim_summaries),
		set(analysis_summaries),
		merged_targets,
		scored_summaries,
		other_tool_targets
		
		
#####################################################
############### simulate integrations ###############
#####################################################


include: "snakemake_rules/simulate_integrations.smk"
include: "snakemake_rules/art.smk"
include: "snakemake_rules/annotate_reads.smk"

#####################################################
################### analyse data ####################
#####################################################

include: "snakemake_rules/preprocessing.smk"
include: "snakemake_rules/alignment.smk"
include: "snakemake_rules/find_ints.smk"
include: "snakemake_rules/postprocessing.smk"


#####################################################
##################### polyidus ######################
#####################################################

include: "snakemake_rules/polyidus.smk"

# overwrite input function to get simulated reads (which are in a different location to other pipeline)
def get_input_reads(wildcards, read_num):
	assert read_num in (1, 2)
	if read_num == 1:
		return f"{wildcards.outpath}/{analysis_df_value(wildcards, 'experiment')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, 'R1_suffix')}"
	if read_num == 2:
		return f"{wildcards.outpath}/{analysis_df_value(wildcards, 'experiment')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, 'R2_suffix')}"	

#####################################################
####################### vifi ########################
#####################################################

include: "snakemake_rules/vifi.smk"

#####################################################
###################### verse ########################
#####################################################

include: "snakemake_rules/verse.smk"

#####################################################
###################### seeksv #######################
#####################################################

include: "snakemake_rules/seeksv.smk"


#####################################################
############ compare sim with analysis ##############
#####################################################


rule write_analysis_summary_all_tools:
	output:
		pipeline_conditions = "{outpath}/{experiment}/pipeline_analysis_conditions.tsv",
		analysis_conditions = "{outpath}/{experiment}/analysis_conditions.tsv"
	run:
		toDo.to_csv(output.pipeline_conditions, sep='\t', index=False)
		analysis_df.to_csv(output.analysis_conditions, sep='\t', index=False)

def get_merged_reads(wildcards):
	# for our pipeline, return result depending on if we merged or not
	if re.search("analysis", wildcards.dset):
		return get_for_align(wildcards, "merged")
	# other pipelines - no merged reads
	else:
		return rules.touch_merged.output.merged
		
rule merged_list:
	input:
		merged = lambda wildcards: get_merged_reads(wildcards)
	output:
		merged_reads = "{outpath}/{dset}/merged_reads/{samp}.merged_reads.txt"
	run:
		if os.path.basename(os.path.dirname(input.merged)) == "combined_reads":
			shell("touch {output.merged_reads}")
		else:
			shell("zcat {input.merged} | awk '{{if (NR%4==1) print substr($0, 2)}}' > {output.merged_reads}")

		
rule score_reads:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		sim_sam = "{outpath}/{exp}/sim_reads/{samp}.sam",
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
		merged_reads = rules.merged_list.output.merged_reads
	output:
		scored_reads = temp("{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}.tsv"),
		scored_reads_uniq = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}.uniq.tsv",
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 50000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	threads: lambda wildcards, attempt: (10, 15, 20, 20)[attempt - 1]
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_reads.py \
		--sim-info {input.sim_info} \
		--sim-sam {input.sim_sam} \
		--analysis-info {input.analysis_info} \
		--merged-reads {input.merged_reads} \
		--output {output.scored_reads} \
		--threads {threads} 
		
		(head -n 1 {output.scored_reads} && tail -n +2 {output.scored_reads} | sort | uniq) > {output.scored_reads_uniq}
		"""
		
rule read_score_summary:
	input:
		scores = rules.score_reads.output.scored_reads_uniq,
		sim_info = rules.annotate_reads.output.annotated_info,
		sim_sam = "{outpath}/{exp}/sim_reads/{samp}.sam",
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
	output:
		summary = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}_summary.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 50000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""	
		echo -e "sim_info_file\tsim_sam_file\tanalysis_info_file\tresults_file\tjunc_type\tscore_type\ttrue_positives\ttrue_negatives\tfalse_positives\tfalse_negatives" > {output.summary}
		
		TP=$(awk -F "\t" '$4 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$4 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$4 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$4 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\tfound_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$4 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$4 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$4 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$4 ~ /fn/ && /chimeric/' {input.scores} | wc -l)

		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\tfound_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}
		
		TP=$(awk -F "\t" '$5 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$5 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$5 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$5 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\thost_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}	

		TP=$(awk -F "\t" '$5 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$5 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$5 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$5 ~ /fn/ && /chimeric/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\thost_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$6 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$6 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t"  '$6 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$6 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\tvirus_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$6 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$6 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$6 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$6 ~ /fn/ && /chimeric/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\tvirus_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		"""

rule combine_read_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "reads")
	output:
		"{outpath}/{exp}/{exp}.scored_reads_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""

rule score_integrations:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
		merged_reads = rules.merged_list.output.merged_reads
	output:
		summary = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}_summary.tsv",
		temp = temp("{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.tmp.tsv"),
		scored_ints = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	params:
		tool = lambda wildcards: f"--analysis-tool {analysis_df_value(wildcards, 'tool')}",
		window = lambda wildcards: f"--window {analysis_df_value(wildcards, 'score_ints_window')}",
	threads: 1
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_integrations.py \
		--sim-info {input.sim_info} \
		--found-info {input.analysis_info} \
		--output {output.temp} \
		--summary {output.summary} \
		{params.tool} {params.window}
		
		python3 scripts/int_fp_type.py \
		--sim-info {input.sim_info} \
		--merged {input.merged_reads} \
		--scored-ints {output.temp} \
		--output {output.scored_ints}
		"""
	
rule score_merged_integrations:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.merged.bed"
	output:
		summary = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.merged_summary.tsv",
		scored_ints = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.merged.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	params:
		tool = lambda wildcards: f"--analysis-tool {analysis_df_value(wildcards, 'tool')}",
		window = lambda wildcards: f"--window {analysis_df_value(wildcards, 'score_ints_window')}",
		merged = "--merged"
	threads: 1
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_integrations.py \
		--sim-info {input.sim_info} \
		--found-info {input.analysis_info} \
		--output {output.scored_ints} \
		--summary {output.summary} \
		{params}
		"""
		
rule combine_int_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "ints")
	output:
		"{outpath}/{exp}/{exp}.scored_ints_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt, input: attempt * len(input) * 10,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""
		
# target files for per-read or per-integration comparison of analysis with simulation
def scores_to_combine(wildcards, score_type):

	assert score_type in ("ints", "reads")
	
	if score_type == "ints":
		folder = "scored_ints"
	else:
		folder = "scored_reads"
	
	scored_files = []
	for i, row in analysis_df.iterrows():
	
		# check if we want to score reads or ints for this row
		if score_type == 'reads':
			if 'score_reads' not in row:
				continue
			if row['score_reads'] != 1:
				continue
		if score_type == 'ints':
			if 'score_ints' not in row and 'score_merged_ints' not in row:
				continue
			if 'score_ints' in row and 'score_merged_ints' in row:
				if row['score_ints'] != 1 and row['score_merged_ints'] != 1:
					continue
			if 'score_ints' in row and 'score_merged_ints' not in row:
				if row['score_ints'] != 1:
					continue
			if 'score_merged_ints' in row and 'score_ints' not in row:
				if row['score_merged_ints'] != 1:
					continue
					
		# only scoring reads for our pipeline
		if score_type == 'reads' and not re.search("analysis", row['analysis_condition']):
			continue
		
		# get information for this row
		exp = row['experiment']

		if exp !=  wildcards.exp:
			continue
		outpath = row['outdir']
		analysis_condition = row['analysis_condition']
		host = row['host']
		virus = row['virus']
	
		# get samples for this experiment
		samples = set(sim_df[sim_df['experiment'] == exp]['sample'])
		
		# we only have merged integrations for integration scoring
		# only do this if the user requested merged integrations to be scored
		merged = ['']
		if score_type == 'ints' and 'score_merged_ints' in row:
			if row['score_merged_ints'] == 1:
				merged = ['.merged', '']
		
		# we only have postprocessed data for our pipeline
		if re.search("analysis", analysis_condition):
			post = ['.post', '']
		else:
			post = ['']
			
		scored_files += [
		f"{outpath}/{exp}/{folder}/{analysis_condition}.{samp}.{host}.{virus}{post}{merged}_summary.tsv"
			for samp, post, merged
			in itertools.product(samples, post, merged)
		]
	
	return scored_files

