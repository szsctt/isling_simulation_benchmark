### snakemake workflow to simulated data, run our pipeline on it, and then analyse the results

import sys
import os
import yaml
import pdb
import copy
import itertools
import pandas as pd
import re
import collections

from python_scripts.parse_config import parse_config
from python_scripts.make_df import make_df, make_post_args, make_reference_dict

from python_scripts.make_df import check_dataset_sample_unique, make_reference_dict, make_post_args, check_fastas_unique

from scripts.parse_analysis_config import parse_analysis_config, get_bool_value_from_config
from scripts.input_functions import get_reads, get_vifi_resource, resources_list_with_min_and_max


# https://sapac.support.illumina.com/bulletins/2016/12/what-sequences-do-i-use-for-adapter-trimming.html
HS25 = {'read1-adapt' : "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA", 
				'read2-adapt' : "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"}		

adapter_seqs = {'HS25':HS25, 'MSv3':HS25, 'MS': HS25}

verse_threads = 8

#defaults for scoring
default_scoring = {'score_ints' : 1, 'score_reads' : 0}
score_type_cols = {
	'shortest' : 7,
	'coords_mean': 8,
	'coords_min': 9,
	'midpoint': 10 
}
score_types = score_type_cols.keys()


#####################################################
############### global parameters ###################
#####################################################

# global parameters (defaults for this dataset) may be specified using a 'global' key
# at the top of the config file.  These will be applied to any missing keys in the remaining 
# datasets in the config file.  If a key is specified in both 'global' and a dataset,
# the value specified in the dataset will be used


def add_default(default, dataset):
	for key in default.keys():

		# if this key is not set for this dataset, add it with default value
		if not (isinstance(default[key], dict) or isinstance(default[key], collections.OrderedDict)):

			if key not in dataset.keys():
				dataset[key] = default[key]
		# otherwise, descend into dict
		else:
			if key not in dataset.keys():
				dataset[key] = {}
			add_default(default[key], dataset[key])

if 'global' in config:		
	# get default (global) options
	default = config.pop('global')
	
	# replace any missing values with default values
	for dataset in config.values():
		add_default(default, dataset)
		

#####################################################
############# simulation parameters #################
#####################################################

# make dataframe with parameters for simulation
config, sim_df, ref_dict = parse_config(config)

# add read_directory key for each dataset (expected by parse_analysis_config function)
for i, row in sim_df.iterrows():
	config[row['experiment']]['read_directory'] = os.path.join(row['out_directory'], row['experiment'], 'sim_reads')
	config[row['experiment']]['out_directory'] = os.path.normpath(row['out_directory'])
	config[row['experiment']]['R1_suffix'] = "1.fq"
	config[row['experiment']]['R2_suffix'] = "2.fq"
	config[row['experiment']]['bam_suffix'] = ".sam"

#####################################################
############## analysis parameters ##################
#####################################################

def convert_bool_to_int(input_bool, dataset, name):
	if isinstance(input_bool, bool):
		if input_bool:
			return 1
		else:
			return 0
	elif isinstance(input_bool, str):
		if input_bool.lower() == "true":
			return 1
		else:
			return 0
	else:
		raise ValueError(f"Unrecgonised input for key {name} in dataset {dataset}")


# make data frame for other tools
analysis_df = parse_analysis_config(config)

# assume that we're doing our pipeline on all datasets
extra_rows = []							
for dataset in config:
	
	#### parameters for our pipeline ####
	# paramters for our pipeline should be lists, so that we can do combinations
	# of each 
	# adding the sequencing system and fragment length in here isn't ideal, because the pipeline will analyse all data in the dataset
	# which may have different fragment lengths, with the same value (and will do this for every sample in the dataset)
	assert hasattr(config[dataset]['merge'], '__iter__')
	assert hasattr(config[dataset]['dedup'], '__iter__')
	assert hasattr(config[dataset]['trim'], '__iter__')
	assert hasattr(config[dataset]['post'], '__iter__')
	assert hasattr(config[dataset]['bwa_mem'], '__iter__')	
	assert hasattr(config[dataset]['seq_sys'], '__iter__')
	assert hasattr(config[dataset]['frag_len'], '__iter__')
	assert hasattr(config[dataset]['merge-dist'], '__iter__')
	assert hasattr(config[dataset]['merge-n-min'], '__iter__')
	assert hasattr(config[dataset]['clip-cutoff'], '__iter__')
	assert hasattr(config[dataset]['cigar-tol'], '__iter__')
	assert hasattr(config[dataset]['min-mapq'], '__iter__')
	
	if len(config[dataset]['seq_sys']) > 1:
		print(f"WARNING: there was more than one sequencing system requested for experiment {dataset}. Each sample in this dataset will be analysed with the appropriate set of adapters for each sequencing system, regardless of whether or not those adapters are appropriate for each sample.  Take care to only use results with matching sequencing systems for analysis and simulation.")
	
	scoring = {}
	for score_type, default in default_scoring.items():
		scoring[score_type] = get_bool_value_from_config(config, dataset, score_type, default)
	if scoring['score_ints'] == 1:
		score_ints_window = config[dataset]['score_ints_window']
	else:
		score_ints_window = 0

	# each combination of these are a unique 'analysis condition' for our pipeline
	i = 0
	for merge, trim, dedup, post, bwa_mem_params, seq_sys, host, virus, merge_dist, merge_n_min, clip_cutoff, cigar_tol, min_mapq in itertools.product(
																		config[dataset]['merge'], 
																		config[dataset]['trim'], 
																		config[dataset]['dedup'],
																		config[dataset]['post'],
																		config[dataset]['bwa_mem'],		
																		config[dataset]['seq_sys'],																
																		config[dataset]['analysis_hosts'].keys(),
																		config[dataset]['analysis_viruses'].keys(),
																		config[dataset]['merge-dist'],
																		config[dataset]['merge-n-min'],
																		config[dataset]['clip-cutoff'],
																		config[dataset]['cigar-tol'],
																		config[dataset]['min-mapq'],
																		):
		
		condition = f"analysis{i}"

		# get adapters
		read1_adapt = adapter_seqs[seq_sys]['read1-adapt']
		read2_adapt = adapter_seqs[seq_sys]['read2-adapt']

		# format parameters required for this condition
		post_args = make_post_args({dataset: {'post':post}})[0][dataset]
		
		# convert merge and dedup to 1 or 0			
		merge = convert_bool_to_int(merge, dataset, 'merge')

		dedup = convert_bool_to_int(dedup, dataset, 'dedup')
		
		trim = convert_bool_to_int(trim, dataset, 'trim')
			
		extra_rows.append({
			'experiment' : dataset,
			'host' 			: host,
			'host_fasta': config[dataset]['analysis_hosts'][host],
			'virus'     : virus,
			'virus_fasta': config[dataset]['analysis_viruses'][virus],
			'analysis_condition': condition,
			'merge'			 : merge,
			'trim'			 : trim,
			'dedup'			 : dedup,
			'postargs'  : make_post_args({dataset: {'post':post}})[0][dataset],
			'adapter_1'  : read1_adapt,
			'adapter_2'  : read2_adapt,
			'bwa_mem_params': bwa_mem_params,
			'clip_cutoff': clip_cutoff,
			'cigar_tol'  : cigar_tol,
			'min_mapq'  : min_mapq,
			'tool'			 : 'pipeline',
			'merge_dist' : merge_dist,
			'merge_n_min': merge_n_min,
			'score_ints_window' : score_ints_window,
			**scoring
		})
		i += 1

	
extra = pd.DataFrame(extra_rows)
analysis_df = pd.concat((extra, analysis_df), axis=0, ignore_index=True)



###### parameters that are common to each experiment ####
analysis_df['bam_suffix'] = ['.sam' for i in range(analysis_df.shape[0])]
analysis_df['R1_suffix'] = ['1.fq' for i in range(analysis_df.shape[0])]
analysis_df['R2_suffix'] = ['2.fq' for i in range(analysis_df.shape[0])]
analysis_df['adapter_1'] = [HS25['read1-adapt'] for i in range(analysis_df.shape[0])]
analysis_df['adapter_2'] = [HS25['read2-adapt'] for i in range(analysis_df.shape[0])]

analysis_df['read_folder'] = [os.path.join(config[dataset]['out_directory'], dataset, "sim_reads") for dataset in analysis_df['experiment']]

analysis_df['outdir'] = [config[dataset]['out_directory'] for dataset in analysis_df['experiment']]

analysis_df['exp'] = analysis_df['experiment']	

# add extra column to analysis_df with a unique identifier for each experiment/analysis condition combo
analysis_df['unique'] = [f"{exp}_{cond}" for exp, cond in zip(analysis_df['experiment'], analysis_df['analysis_condition'])]

# also add columns indicating if we want to score integrations, merged integrations, and reads

# rows to add 'score_reads' 'score_ints', 'score_merged_ints', 'score_ints_window',  for other tools
# we only score reads from our pipeline

score_reads = []
score_ints = []
score_ints_window = []
score_ints_type = []
for i, row in analysis_df.iterrows():
	# only score reads from our pipeline
	if row['tool'] != 'pipeline':
		score_reads.append(0)
	else:
		score_reads.append(get_bool_value_from_config(config, row['experiment'], 'score_reads', default_scoring['score_reads']))
	
	# scoring ints
	score_ints.append(get_bool_value_from_config(config, row['experiment'], 
																							'score_ints', default_scoring['score_ints']))
																							
	# get score_ints_window if appropriate
	if score_ints[i] == 1 or score_merged_ints[i] == 1:
		score_ints_window.append(config[row['experiment']]['score_ints_window'])
		score_ints_type.append(config[row['experiment']]['score_ints_type'])
	else:
		score_ints_window.append(None)
		score_ints_type.append(None)
		
analysis_df['score_reads'] = score_reads
analysis_df['score_ints'] = score_ints
analysis_df['score_ints_window'] = analysis_df['score_ints_window'].astype('object')
analysis_df['score_ints_window'] = score_ints_window
analysis_df['score_ints_type'] = score_ints_type

## our pipeline also requires a dataframe called 'toDo', containing the analysis paramters for each sample in each dataset.  Build this using sim_df (from simulation) and analysis_df (analysis parameters).  This must have the following columns:

column_names = ['dataset', 'config_dataset', 'sample', 'host', 'host_fasta', 'host_prefix', 'virus', 'virus_fasta', 'virus_prefix', 'merge', 'trim', 'dedup', 'unique', 'outdir', 'bwa_mem_params', 'R1_file', 'R2_file', 'bam_file', 'adapter_1', 'adapter_2', 'postargs', 'merge_dist', 'merge_n_min', 'clip_cutoff', 'cigar_tol', 'min_mapq', 'split', 'mean_frag_len', 'align_cpus', 'cat', 'split_lines']

rows = []

# build rows of toDo
for i, row in analysis_df.iterrows():

	# skip any rows we don't want to analyse with our pipeline
	if row['tool'] != 'pipeline':
		continue

	# get samples for this simulated dataset
	experiment = row['experiment']
	dataset = row['unique']
	samples = sim_df[sim_df['experiment'] == experiment]['sample']
	
	# make a row for each sample in this dataset
	for sample in samples:
		
		# use information we already worked out from config
		todo_row = dict(row)
		todo_row['dataset'] = dataset
		todo_row['config_dataset'] = experiment
		
		# add sample name
		todo_row['sample'] = sample
		todo_row['unique'] = f"{dataset}+++{sample}"
		
		# get the correct adapters and fragment length for this sample
		exp_df = sim_df[sim_df['experiment'] == experiment]
		seq_sys = exp_df[exp_df['sample'] == sample]['seq_sys'].tolist()[0]
		todo_row['adapter_1'] = adapter_seqs[seq_sys]['read1-adapt']
		todo_row['adapter_2'] = adapter_seqs[seq_sys]['read2-adapt']
		
		# get the mean fragment length used for simulation
		todo_row['mean_frag_len'] = exp_df[exp_df['sample'] == sample]['frag_len'].tolist()[0]
		
		# read file names
		read_prefix = os.path.join(row['outdir'], experiment, 'sim_reads', sample)
		todo_row['R1_file'] = read_prefix + row['R1_suffix']
		todo_row['R2_file'] = read_prefix + row['R2_suffix']
		todo_row['bam_file'] = read_prefix + row['bam_suffix']
		
		# host and virus prefixes
		todo_row['host_prefix'] = os.path.join(row['outdir'], 'references', row['host'], row['host'])
		todo_row['virus_prefix'] = os.path.join(row['outdir'], 'references', row['virus'], row['virus'])
		
		# number of cpus for alignments
		todo_row['align_cpus'] = verse_threads

		todo_row['cat'] = 'cat'
		todo_row['split'] = 1
		todo_row['split_lines'] = ''
		
		rows.append(todo_row)
		
toDo = pd.DataFrame(rows, columns = column_names)

# make dictionary of reference names and fasta files
ref_names = make_reference_dict(toDo)

# construct arguments for postprocess.R script for each dataset

POSTARGS, TOSORT, SORTED = make_post_args(config)

#####################################################
############ wildcard constraints ###################
#####################################################

wildcard_constraints:
	cond = "|".join(set(sim_df.loc[:, 'condition'])),
	exp = "|".join(set(sim_df.loc[:, 'experiment'])),
	rep = "|".join(set([str(i) for i in sim_df.loc[:, 'replicate']])),
	dset = "|".join(set(analysis_df.loc[:, 'unique'])),
	samp = "|".join(set(toDo.loc[:, 'sample'])),
	host = "|".join(set(analysis_df.loc[:, 'host'])),
	virus = "|".join(set(analysis_df.loc[:, 'virus'])),
	post = "|\.post",
	merged = "|\.merged",
	outpath = "|".join(set(analysis_df.loc[:, 'outdir'])),
	dist = "\d+",
	score_type = "|".join(score_types),
	part = "\d+",
	pad_len = "\d+"

dset_non_isling = ".+"
#dset_non_isling = ".+_((vifi)|(seeksv)|(polyidus))\d+"

localrules: write_summary, write_analysis_summary, write_analysis_summary_all_tools

#####################################################
################## ruleorder ########################
#####################################################

# to resolve conflicts between polyidus and combine_ints (our analysis pipeline)
# rule polyidus has a wildcard contstraint so it can't be used for our pipeline
# so make it higher in the priority

ruleorder: vifi > polyidus > verse > seeksv > combine_ints > merged_bed > merged_list > touch_merged_reads_other_tools

#####################################################
################### target files ####################
#####################################################

# target files for simulation
sim_targets = sim_df.loc[:,'annotated_info_filename']

# summaries about how experiment was conducted
exp_targets = {}
for i, row in sim_df.iterrows():
	exp_targets[row['experiment']] = row['out_directory']
experiments = list(exp_targets.keys())

sim_summaries = expand("{outpath}/{exp}/simulation_summary.tsv", 
			zip,
			exp = experiments, 
			outpath = [exp_targets[exp] for exp in experiments]
			)

analysis_summaries = expand("{outpath}/{exp}/analysis_conditions.tsv", 
			zip,
			exp = experiments, 
			outpath = [exp_targets[exp] for exp in experiments]
			)

# outputs from our pipeline
merged_targets = set()
for i, row in analysis_df.iterrows():
	if row['tool'] != 'pipeline':
		continue
	
	host = row['host']
	virus = row['virus']		
	outpath = row['outdir']
	dset = row['unique']
		
	samples = sim_df[sim_df['experiment'] == row['experiment']]['sample']

		
	for samp, post in itertools.product(samples, (".post", "")):
		merged_targets.add(
			f"{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.merged.txt"
		)

# targets for other tools
other_tool_targets = set()
for i, row in analysis_df.iterrows():
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['unique']
	host = row['host']
	virus = row['virus']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
		
	for samp in samples:
		if row['tool'] == 'vifi':
			other_tool_targets.add(
				f"{outpath}/{dset}/vifi/{samp}.{host}.{virus}/output.clusters.txt"
			)
		elif row['tool'] == 'polyidus':
			other_tool_targets.add(
				f"{outpath}/{dset}/polyidus/{host}.{virus}.{samp}/results/exactHpvIntegrations.tsv"
			)
		elif row['tool'] == 'seeksv':
			other_tool_targets.add(
				f"{outpath}/{dset}/seeksv/ints/{samp}.{host}.{virus}.integrations.txt"
			)			


# scored reads
scored_summaries = set()
for i, row in analysis_df.iterrows():
	if row['score_ints'] == 1:
		scored_summaries.add(f"{row['outdir']}/{row['experiment']}/{row['experiment']}.scored_ints_summary.tsv")
	if 'score_reads' in row:
		if row['score_reads'] == 1:
			scored_summaries.add(f"{row['outdir']}/{row['experiment']}/{row['experiment']}.scored_reads_summary.tsv")
	if row['tool'] == 'pipeline':
		scored_summaries.add(f"{row['outdir']}/{row['experiment']}/{row['experiment']}.jaccard_summary.tsv")

rule all:
	input: 
		set(sim_targets),
		set(sim_summaries),
		set(analysis_summaries),
#		merged_targets,
		scored_summaries,
		other_tool_targets


#####################################################
############### simulate integrations ###############
#####################################################

include: "snakemake_rules/simulate_integrations.smk"
include: "snakemake_rules/art.smk"
include: "snakemake_rules/annotate_reads.smk"

#####################################################
################### analyse data ####################
#####################################################

include: "snakemake_rules/preprocessing.smk"
include: "snakemake_rules/alignment.smk"
include: "snakemake_rules/find_ints.smk"
include: "snakemake_rules/postprocessing.smk"

#####################################################
##################### polyidus ######################
#####################################################

include: "snakemake_rules/polyidus.smk"

# overwrite input function to get simulated reads (which are in a different location to other pipeline)
def get_reads(wildcards, analysis_df, rules, read_num):
	assert read_num in {1, 2}
	trim = bool(analysis_df_value(wildcards, analysis_df, 'trim'))

	if read_num == 1:
		if trim is True:
			return f"{wildcards.outpath}/{dset}/trimmed_reads/{wildcards.samp}.1.fastq.gz"
		else:
			return f"{wildcards.outpath}/{analysis_df_value(wildcards, analysis_df, 'experiment')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, analysis_df, 'R1_suffix')}"
		
	if read_num == 2:
		if trim is True:
			return f"{wildcards.outpath}/{dset}/trimmed_reads/{wildcards.samp}.2.fastq.gz"
		else:
			return f"{wildcards.outpath}/{analysis_df_value(wildcards, analysis_df, 'experiment')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, analysis_df, 'R2_suffix')}"	
		
def analysis_df_value(wildcards, analysis_df, column_name):
	"""Get resources required for a dataset (but not tool-specific)"""

	row = analysis_df[(analysis_df['unique'] == wildcards.dset)].index[0]
	return analysis_df.loc[row, column_name]

def analysis_df_tool_value(wildcards, analysis_df, tool, column_name):
	"""Get resources required for a dataset"""
	row = analysis_df[(analysis_df['unique'] == wildcards.dset) & (analysis_df['tool'] == tool)].index[0]
	return analysis_df.loc[row, column_name]


#####################################################
##################### trimming ######################
#####################################################

# we need a different rule for trimming than the one in preprocessing.smk
# because that one gets adapters from toDo, which for now only has 
# parameters for our pipeline in it

def get_adapters(wildcards, num):
	# get adapters depending on sequencing system used for this 'sample'
	assert num in (1, 2)

	# get sequencing system for this experiment/sample
	experiment = "_".join(wildcards.dset.split("_")[:-1])
	unique = f"{experiment}__{wildcards.samp}"
	seq_sys = sim_df.loc[(sim_df['unique'] == unique).idxmax(), 'seq_sys']
	
	# get adapter for this sequencing system
	if num == 1:
		return adapter_seqs[seq_sys]['read1-adapt']
	else:
		return adapter_seqs[seq_sys]['read2-adapt']		

def get_input_reads(wildcards, analysis_df, read_num):
	assert read_num in (1, 2)
	readdir = analysis_df_value(wildcards, analysis_df, 'read_folder')
	if read_num == 1:
		suffix = analysis_df_value(wildcards, analysis_df, 'R1_suffix')
	else:
		suffix = analysis_df_value(wildcards, analysis_df, 'R2_suffix')		
	return os.path.join(readdir, f"{wildcards.samp}{suffix}")

	
def get_reads(wildcards, analysis_df, rules, read_num):
	assert read_num in (1, 2)
	if analysis_df_value(wildcards, analysis_df, 'trim') == 1:
		if read_num == 1:
			return rules.trim.output.proc_r1
		else:
			return rules.trim.output.proc_r2
	else:
		if read_num == 1:
			return get_input_reads(wildcards, analysis_df, 1)
		else:
			return get_input_reads(wildcards, analysis_df, 2)		

rule trim:
	input:
		r1 = lambda wildcards: get_input_reads(wildcards, analysis_df, 1),
		r2 = lambda wildcards: get_input_reads(wildcards, analysis_df, 2)
	output:
		proc_r1 = temp("{outpath}/{dset}/trimmed_reads/{samp}.1.fastq.gz"),
		proc_r2 = temp("{outpath}/{dset}/trimmed_reads/{samp}.2.fastq.gz")
	resources:
		mem_mb= lambda wildcards, attempt: int(attempt * 5000),
		time = lambda wildcards, attempt: ('30:00', '2:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	wildcard_constraints:
		dset = dset_non_isling
	conda:	
		"../envs/seqprep.yml"
	container:
		"docker://szsctt/seqprep:1"

	params:
		A = lambda wildcards: get_adapters(wildcards, 1),
		B = lambda wildcards: get_adapters(wildcards, 2)
	shell:
		"""
		SeqPrep -A {params.A} -B {params.B} -f {input.r1} -r {input.r2} -1 {output.proc_r1} -2 {output.proc_r2}
		"""

rule touch_merged_reads_other_tools:
	output:
		"{outpath}/{dset}/merged_reads/{samp}.merged_reads.txt"
	wildcard_constraints:
		dset = dset_non_isling
	shell:
		"""
		touch {output}
		"""	

#####################################################
####################### vifi ########################
#####################################################

include: "snakemake_rules/vifi.smk"

#####################################################
###################### verse ########################
#####################################################

include: "snakemake_rules/verse.smk"

#####################################################
###################### seeksv #######################
#####################################################

include: "snakemake_rules/seeksv.smk"


#####################################################
################### VSeq-Toolkit ####################
#####################################################

include: "snakemake_rules/vseq.smk"



#####################################################
############ compare sim with analysis ##############
#####################################################

rule write_analysis_summary_all_tools:
	output:
		pipeline_conditions = "{outpath}/{experiment}/pipeline_analysis_conditions.tsv",
		analysis_conditions = "{outpath}/{experiment}/analysis_conditions.tsv"
	run:
		toDo.to_csv(output.pipeline_conditions, sep='\t', index=False)
		analysis_df.to_csv(output.analysis_conditions, sep='\t', index=False)



# need to redefine this function to refer to 'unique' column
def analysis_df_value(wildcards, analysis_df, column_name):
	return analysis_df.loc[(analysis_df['unique'] == wildcards.dset).idxmax(), column_name]	

def get_merged_reads(wildcards):
	n_parts = toDo.loc[(toDo['unique'] == f"{wildcards.dset}+++{wildcards.samp}").idxmax(), 'split']	
	
	# for our pipeline, return result depending on if we merged R1 and R2 or not
	files = []
	for n in range(n_parts):
		files.append(get_for_align(wildcards, "merged").replace('{part}', str(n)))
	
	return files

		
rule merged_list:
	input:
		merged = lambda wildcards: get_merged_reads(wildcards)
	output:
		merged_reads = "{outpath}/{dset}/merged_reads/{samp}.merged_reads.txt"
	wildcard_constraints:
		dset = ".+_analysis\d+"	
	run:
		if os.path.basename(os.path.dirname(input.merged[0])) == "combined_reads":
			shell("touch {output.merged_reads}")
		else:
			shell("zcat {input.merged} | awk '{{if (NR%4==1) print substr($0, 2)}}' > {output.merged_reads}")
	
rule score_reads:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		sim_sam = "{outpath}/{exp}/sim_reads/{samp}.sam",
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
		merged_reads = rules.merged_list.output.merged_reads
	output:
		scored_reads = temp("{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}.tsv"),
		scored_reads_uniq = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}.uniq.tsv",
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 50000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	threads: lambda wildcards, attempt: (10, 15, 20, 20)[attempt - 1]
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_reads.py \
		--sim-info {input.sim_info} \
		--sim-sam {input.sim_sam} \
		--analysis-info {input.analysis_info} \
		--merged-reads {input.merged_reads} \
		--output {output.scored_reads} \
		--threads {threads} 
		
		(head -n 1 {output.scored_reads} && tail -n +2 {output.scored_reads} | sort | uniq) > {output.scored_reads_uniq}
		"""
		
rule read_score_summary:
	input:
		scores = rules.score_reads.output.scored_reads_uniq,
		sim_info = rules.annotate_reads.output.annotated_info,
		sim_sam = "{outpath}/{exp}/sim_reads/{samp}.sam",
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
	output:
		summary = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}_summary.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 50000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""	
		echo -e "sim_info_file\tsim_sam_file\tanalysis_info_file\tresults_file\tjunc_type\tscore_type\ttrue_positives\ttrue_negatives\tfalse_positives\tfalse_negatives" > {output.summary}
		
		TP=$(awk -F "\t" '$4 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$4 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$4 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$4 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\tfound_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$4 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$4 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$4 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$4 ~ /fn/ && /chimeric/' {input.scores} | wc -l)

		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\tfound_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}
		
		TP=$(awk -F "\t" '$5 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$5 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$5 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$5 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\thost_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}	

		TP=$(awk -F "\t" '$5 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$5 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$5 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$5 ~ /fn/ && /chimeric/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\thost_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$6 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$6 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t"  '$6 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$6 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\tvirus_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$6 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$6 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$6 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$6 ~ /fn/ && /chimeric/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\tvirus_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		"""

rule combine_read_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "reads")
	output:
		"{outpath}/{exp}/{exp}.scored_reads_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""

rule bedtools_closest:
	input:
		results_bed = lambda wildcards: get_bed_result(wildcards, padding = 0),
		sim_bed = rules.sim_bed.output.bed
	output:
		closest = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}.{dist}.{score_type}{post}_{type}-results.tsv"
	wildcard_constraints:
		found = "found|sim"
	container:
		"docker://szsctt/bedtools:1"	
	resources:
		mem_mb= lambda wildcards, attempt, input: 1000,
		time = lambda wildcards, attempt: ('30:00', '2:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1	
	params:
		a_file = lambda wildcards, input: input.results_bed if wildcards.type == "found" else input.sim_bed,
		b_file = lambda wildcards, input: input.sim_bed if wildcards.type == "found" else input.results_bed,
	shell:
		"""
		bedtools closest -d -a {params.a_file} -b {params.b_file} | \
		python3 scripts/add_cols.py > {output.closest}
		"""		

rule score_ints:
	input:
		sim_to_found = expand(rules.bedtools_closest.output.closest, type="sim", allow_missing=True),
		found_to_sim =  expand(rules.bedtools_closest.output.closest, type="found", allow_missing=True),
	output:
		summary = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}.{dist}.{score_type}{post}_summary.tsv",
	params:
		tool = lambda wildcards: analysis_df_value(wildcards, analysis_df, 'tool'),
	container:
		"docker://szsctt/simvi:2"
	shell:
		"""
		python3 scripts/summarise_ints.py \
		 --sim-to-found {input.sim_to_found} \
		 --found-to-sim {input.found_to_sim} \
		 --output {output.summary} \
		 --threshold {wildcards.dist} \
		 --threshold-col {wildcards.score_type} \
		 --tool {params.tool}
		"""
		
		
		
# target files for per-read or per-integration comparison of analysis with simulation
def scores_to_combine(wildcards, score_type):

	assert score_type in ("ints", "reads", "jaccard")

	scored_files = []
	for i, row in analysis_df.iterrows():
	
		# check if we want to score reads or ints for this row
		if score_type == 'reads':
			if row['score_reads'] != 1:
				continue
		if score_type == 'ints':
			if row['score_ints'] != 1:
				continue

		# only scoring reads for our pipeline
		if score_type == 'reads' and not re.search("analysis", row['analysis_condition']):
			continue

		
		# get information for this row
		exp = row['experiment']

		if exp !=  wildcards.exp:
			continue
		outpath = row['outdir']
		unique = row['unique']
		host = row['host']
		virus = row['virus']
		dset = f"{row['experiment']}_{row['analysis_condition']}"
	
		# get samples for this experiment
		samples = set(sim_df[sim_df['experiment'] == exp]['sample'])
		
		# we only have postprocessed data for our pipeline
		if re.search("analysis", unique):
			post = '.post'
		else:
			post = ''
	
		# get distances for this experiment - will only be used for jaccard and score_ints
		dists = analysis_df[analysis_df['experiment'] == wildcards.exp]['score_ints_window']
		row_num = dists.index[0]
		assert all([entry == dists[row_num] for entry in dists])
		dists = dists[row_num]
		if not hasattr(dists, '__iter__'):
			dists = [dists]
	
		if score_type == "ints":
			
			# get score types for this experiment
			types = analysis_df[analysis_df['experiment'] == wildcards.exp]['score_ints_type']
			row_num = types.index[0]
			assert all([entry == types[row_num] for entry in types])
			types = types[row_num]
			if not hasattr(types, '__iter__'):
				types = [types]	
			
			# add scored
			scored_files += [
			f"{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}.{dist}.{score_type}{post}_summary.tsv"
				for samp, dist, score_type
				in itertools.product(samples, dists, types)
			]
		elif score_type == 'reads':
			scored_files += [
			f"{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}_summary.tsv"
				for samp in samples
			]	
		else:
			scored_files += [
			f"{outpath}/{exp}/jaccard/{dset}.{samp}.{host}.{virus}{post}.pad{dist}.txt"
				for samp, dist in itertools.product(samples, dists)
			]

	
	return scored_files

rule combine_int_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "ints")
	output:
		"{outpath}/{exp}/{exp}.scored_ints_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt, input: attempt * len(input) * 10,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""	

# jaccard
rule make_isling_bed:
	input:
		merged = rules.merged_bed.output.merged
	output:
		bed = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.merged.bed"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt, input: 100,
		time = lambda wildcards, attempt: ('10:00', '30:00', '2:00:00', '24:00:00')[attempt - 1],
		nodes = 1
	wildcard_constraints:
		dset = ".+_analysis\d+"
	shell:
		"""
		awk '(NR != 1) {{print $1"\t"$2"\t"$3}}' {input.merged} | sort -k1,1 -k2,2n > {output.bed}
		"""

rule make_vifi_bed:
	input:
		range = rules.vifi.output.range
	output:
		bed = "{outpath}/{dset}/vifi/{samp}.{host}.{virus}/output.clusters.txt.range.bed"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt, input: 100,
		time = lambda wildcards, attempt: ('10:00', '30:00', '2:00:00', '24:00:00')[attempt - 1],
		nodes = 1
	wildcard_constraints:
		dset = ".+_vifi\d+"
	shell:
		"""
		awk -F, '(NR != 1) {{print $1"\t"$2"\t"$3}}' {input.range} | sort -k1,1 -k2,2n > {output.bed}
		"""		

rule make_polyidus_bed:
	input:
		in_bed = rules.polyidus.output.bed	
	output:
		out_bed = "{outpath}/{dset}/polyidus/{host}.{virus}.{samp}/results/exactHpvIntegrations.simple.bed"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt, input: 100,
		time = lambda wildcards, attempt: ('10:00', '30:00', '2:00:00', '24:00:00')[attempt - 1],
		nodes = 1
	wildcard_constraints:
		dset = ".+_polyidus\d+"
	shell:
		"""
		awk '(NR != 1) {{print $1"\t"$2"\t"$3}}' {input.in_bed} | sort -k1,1 -k2,2n > {output.out_bed}
		"""	

rule make_seeksv_bed:
	input:
		ints = rules.seeksv.output.ints,
		fai = rules.vifi_faidx.output.fai
	output:
		tmp = temp("{outpath}/{dset}/seeksv/ints/{samp}.{host}.{virus}.integrations.bed.tmp"),
		bed = "{outpath}/{dset}/seeksv/ints/{samp}.{host}.{virus}.integrations.bed",
		chromlist = "{outpath}/{dset}/seeksv/ints/{samp}.{host}.{virus}.chroms.tmp"
	conda:
		"../envs/simvi.yml"
	container:
		"docker://szsctt/simvi:2"
	resources:
		mem_mb= lambda wildcards, attempt, input: 100,
		time = lambda wildcards, attempt: ('10:00', '30:00', '2:00:00', '24:00:00')[attempt - 1],
		nodes = 1	
	wildcard_constraints:
		dset = ".+_seeksv\d+"
	shell:
		"""
		awk 'BEGIN {{ ORS = " " }} {{a[$1]}} END {{for (i in a) print i}}' \
			{input.fai} > {output.chromlist}
		python3 scripts/write_seeksv_bed.py --seeksv-output {input.ints} --chromlist {output.chromlist} --output {output.tmp}
		sort -k1,1 -k2,2n {output.tmp} > {output.bed}
		"""	

rule make_vseq_bed:
	input:
		ints = rules.vseq_toolkit.output.unique
	output:
		bed = "{outpath}/{dset}/vseq_toolkit/{samp}.{host}.{virus}/ISGenomeVector.bed",
		tmp =  temp("{outpath}/{dset}/vseq_toolkit/{samp}.{host}.{virus}/ISGenomeVector.bed.tmp"),
#	wildcard_constraints:
#		dset = ".+_vseq_toolkit\d+"
	shell:
		"""
		python3 scripts/write_vseq-toolkit_bed.py --vseq-toolkit-output {input.ints} --output {output.tmp}
		sort -k1,1 -k2,2n {output.tmp} > {output.bed}
		"""

def get_bed_result(wildcards, padding = 0):

	if analysis_df_value(wildcards, analysis_df, 'tool') == 'pipeline':
		bed = rules.make_isling_bed.output.bed
			
	if analysis_df_value(wildcards, analysis_df, 'tool') == 'vifi':
		bed = rules.make_vifi_bed.output.bed
		
	if analysis_df_value(wildcards, analysis_df, 'tool') == 'polyidus':
		bed =  rules.make_polyidus_bed.output.out_bed		
	
	if analysis_df_value(wildcards, analysis_df, 'tool') == 'seeksv':
		bed = rules.make_seeksv_bed.output.bed	
	
	if analysis_df_value(wildcards, analysis_df, 'tool') == 'vseq_toolkit':
		bed = rules.make_vseq_bed.output.bed	
	
	if padding == 0:
		return bed
	else:
		base = os.path.splitext(bed)[0]
		return base + f".pad{padding}.bed"

rule pad_bed:
	input:
		bed = "{input_file}.bed",
	output:
		bed = "{input_file}.pad{pad_len}.bed"
	resources:
		mem_mb= lambda wildcards, attempt: 1000 * attempt,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1		
	run:	
		with open(input.bed, 'r') as in_file, open(output.bed, 'w') as out_file:
			for line in in_file:
				bed_line = line.strip().split()
				bed_line[1] = str(int(bed_line[1]) - int(wildcards.pad_len))
				bed_line[2] = str(int(bed_line[2]) + int(wildcards.pad_len))
				out_file.write("\t".join(bed_line) + "\n")				
				
	
rule jaccard:
	input:
		results_bed = lambda wildcards: get_bed_result(wildcards, padding = wildcards.pad_len),
		sim_bed = "{outpath}/{exp}/sim_ints/{samp}.int-info.pad{pad_len}.bed"
	output:
		jaccard = "{outpath}/{exp}/jaccard/{dset}.{samp}.{host}.{virus}{post}.pad{pad_len}.txt"
	container:
		"docker://szsctt/bedtools:1"	
	resources:
		mem_mb= lambda wildcards, attempt, input: 1000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1		
	shell:
		"""
		echo "{input.results_bed}	{input.sim_bed}	$(bedtools jaccard -a {input.sim_bed} -b {input.results_bed} | tail -n1)" > {output.jaccard}
		"""

rule combine_jaccard:
	input:
		lambda wildcards: scores_to_combine(wildcards, "jaccard")
	output:
		"{outpath}/{exp}/{exp}.jaccard_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt, input: attempt * len(input) * 10,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		echo "results_bed\tsim_bed\t"
		cat {input} > {output}
		"""	
