### snakemake workflow to simulated data, run our pipeline on it, and then analyse the results

import sys
import os
import yaml
import pdb
import copy
import itertools
import pandas as pd
import re

from python_scripts.parse_config import parse_config
from python_scripts.make_df import make_df, make_post_args, make_reference_dict

from python_scripts.make_df import check_dataset_sample_unique, make_reference_dict, make_post_args, check_fastas_unique


# https://sapac.support.illumina.com/bulletins/2016/12/what-sequences-do-i-use-for-adapter-trimming.html
HS25 = {'read1-adapt' : "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA", 
				'read2-adapt' : "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"}		

adapter_seqs = {'HS25':HS25, 'MSv3':HS25, 'MS': HS25}

#####################################################
############### global parameters ###################
#####################################################

# global parameters (defaults for this dataset) may be specified using a 'global' key
# at the top of the config file.  These will be applied to any missing keys in the remaining 
# datasets in the config file.  If a key is specified in both 'global' and a dataset,
# the value specified in the dataset will be used

if 'global' in config:		
	# get default (global) options
	default = config.pop('global')
	for dataset in config:
		for key in default:
			if key not in config[dataset]:
				config[dataset][key] = default[key]
			


#####################################################
############# simulation parameters #################
#####################################################


# make dataframe with parameters for simulation
config, sim_df, ref_dict = parse_config(config)


#####################################################
############## analysis parameters ##################
#####################################################

def get_bool_value_from_config(config, dataset, key, default):
	if key not in config[dataset]:
		return default
		
	if config[dataset][key] is True:
		return 1
	elif config[dataset][key] is False:
		return 0
	else:
		raise ValueError(f"Boolean value for {key} in dataset {dataset} is neither True nor False.  Please specify one or the other")
	

# get unique analysis conditions - these are combinations of the analysis parameters that
# can be set in our pipeline (merging, de-duplicaiton, bwa mem prarameters, etc), or
# the tool to be used in analysis

analysis_conditions = []
datasets = list(config.keys())

column_names = ('experiment', 'exp', 'analysis_condition', 'tool', 'host', 'host_fasta',
								'virus', 'virus_fasta', 'bam_suffix',
								'read_folder', 'R1_suffix', 'R2_suffix', 'outdir', 
								'merge', 'trim', 'dedup', 'postargs', 'seq_sys', 'adapter_1', 'adapter_2', 
								'bwa_mem_params', 'merge_dist', 'merge_n_min', 'clip_cutoff', 'cigar_tol', 'min_mapq', 									'score_ints', 'score_merged_ints', 'score_ints_window', 'score_ints_tool',
								'host_mappability', 'host_mappability_exclude', 'host_genes', 'host_exons',
								'host_oncogenes', 'host_centromeres', 'host_conserved_regions',
								'host_segdup', 'detection_mode', 'flank_region_size', 'sensitivity_level', 
								'min_contig_length', 'blastn_evalue_thrd', 'similarity_thrd', 
								'chop_read_length', 'minIdentity')																


for dataset in datasets:
	
		#### parameters for comparison of analysis and simulation ####	
	# are we doing per-read comparison of simulation with results?
	score_reads = get_bool_value_from_config(config, dataset, 'score_reads', 0)
	

	# are we doing per-integration comparison of simulation with results?	
	score_ints = get_bool_value_from_config(config, dataset, 'score_ints', 1)
	score_merged_ints = get_bool_value_from_config(config, dataset, 'score_ints', 1)
			
	# check if we've specified a window around simulated integrations to look
	score_ints_window = None	
	if 'score_ints_window' in config[dataset]:
		score_ints_window = config[dataset]['score_ints_window']			
			
	# double check window is specified if score_ints or score_merged_ints is True
	if score_ints == 1 or score_merged_ints == 1:
		if score_ints_window is None:
			raise ValueError(f"score_ints or score_merged_ints was True for dataset {dataset}, but score_ints_window was not specified")
	
	#### parameters for our pipeline ####
	# paramters for our pipeline should be lists, so that we can do combinations
	# of each 
	assert hasattr(config[dataset]['merge'], '__iter__')
	assert hasattr(config[dataset]['dedup'], '__iter__')
	assert hasattr(config[dataset]['trim'], '__iter__')
	assert hasattr(config[dataset]['post'], '__iter__')
	assert hasattr(config[dataset]['bwa_mem'], '__iter__')	
	assert hasattr(config[dataset]['seq_sys'], '__iter__')
	assert hasattr(config[dataset]['merge-dist'], '__iter__')
	assert hasattr(config[dataset]['merge-n-min'], '__iter__')
	assert hasattr(config[dataset]['clip-cutoff'], '__iter__')
	assert hasattr(config[dataset]['cigar-tol'], '__iter__')
	assert hasattr(config[dataset]['min-mapq'], '__iter__')

	# each combination of these are a unique 'analysis condition' for our pipeline
	i = 0
	for merge, trim, dedup, post, bwa_mem_params, seq_sys, host, virus, merge_dist, merge_n_min, clip_cutoff, cigar_tol, min_mapq in itertools.product(
																		config[dataset]['merge'], 
																		config[dataset]['trim'], 
																		config[dataset]['dedup'],
																		config[dataset]['post'],
																		config[dataset]['bwa_mem'],
																		config[dataset]['seq_sys'],
																		config[dataset]['analysis_hosts'].keys(),
																		config[dataset]['analysis_viruses'].keys(),
																		config[dataset]['merge-dist'],
																		config[dataset]['merge-n-min'],
																		config[dataset]['clip-cutoff'],
																		config[dataset]['cigar-tol'],
																		config[dataset]['min-mapq'],
																		):
		
		condition = f"{dataset}_analysis{i}"

		# format parameters required for this condition
		post_args = make_post_args({dataset: {'post':post}})[0][dataset]
		
		# figure out which adapters to use, based on which sequencing system used for art
		read1_adapt = adapter_seqs[seq_sys]['read1-adapt']
		read2_adapt = adapter_seqs[seq_sys]['read2-adapt']
		
		# convert merge and dedup to 1 or 0
		if merge is True or merge == 'true' or merge == 'True':
			merge = 1
		else:
			merge = 0
			
		if dedup is True or dedup == 'true' or dedup == 'True':
			dedup = 1
		else:
			dedup = 0

		analysis_conditions.append({
			'experiment' : dataset,
			'host' 			: host,
			'host_fasta': config[dataset]['analysis_hosts'][host],
			'virus'     : virus,
			'virus_fasta': config[dataset]['analysis_viruses'][virus],
			'analysis_condition': condition,
			'merge'			 : merge,
			'trim'			 : trim,
			'dedup'			 : dedup,
			'postargs'  : make_post_args({dataset: {'post':post}})[0][dataset],
			'adapter_1'  : read1_adapt,
			'adapter_2'  : read2_adapt,
			'bwa_mem_params': bwa_mem_params,
			'clip_cutoff': clip_cutoff,
			'cigar_tol'  : cigar_tol,
			'min_mapq'  : min_mapq,
			'tool'			 : 'pipeline',
			'merge_dist' : merge_dist,
			'merge_n_min': merge_n_min,
			'score_reads' : score_reads,
			'score_ints' : score_ints,
			'score_merged_ints' : score_merged_ints,
			'score_ints_window' : score_ints_window,
			'score_ints_tool' : "pipeline"	
		})
		i += 1
	
	#### parameters for polyidus ####
	i = 0
	if 'polyidus_params' in config[dataset]:
			
			# are we trying multiple aligners?
			if 'aligner' in config[dataset]['polyidus_params']:
				assert hasattr(config[dataset]['polyidus_params']['aligner'], '__iter__')
				aligners = config[dataset]['polyidus_params']['aligner']
			else:
				aligners = ['bowtie2']
				
			for host, virus, aligner in itertools.product(
																		config[dataset]['analysis_hosts'].keys(),
																		config[dataset]['analysis_viruses'].keys(),
																		aligners):
				# give this analysis condition a name
				condition = f"{dataset}_polyidus{i}"

				analysis_conditions.append({
					'experiment' : dataset,
					'host' 			 : host,
					'host_fasta' : config[dataset]['analysis_hosts'][host],
					'virus'      : virus,
					'virus_fasta': config[dataset]['analysis_viruses'][virus],
					'analysis_condition': condition,
					'aligner'		 : aligner,
					'merge'			 : 0,
					'tool'			 : 'polyidus',
					'score_reads' : score_reads,
					'score_ints' : score_ints,
					'score_merged_ints' : score_merged_ints,
					'score_ints_window' : score_ints_window,
					'score_ints_tool' : "polyidus"	
					})	
				i += 1
	
	#### parameters for vifi ####
	# if we're also doing vifi
	i = 0
	if 'vifi_params' in config[dataset]:
		# make sure the required information about the host genome has been provided
		host_file_keys = ('mappability', 'mappability_exclude', 'genes', 'exons', 'oncogenes', 'centromeres',
												'conserved_regions', 'segdup')
		hosts_to_use = []
		for host in config[dataset]['analysis_hosts'].keys():
			# check that this host is in host_info
			if host not in config[dataset]['vifi_params']['host_info']:
				print(f"host_info not provided: skipping ViFi for {host}")
				continue
			# check that all necessary files have been specified
			if not all([key in config[dataset]['vifi_params']['host_info'][host] for key in host_file_keys]):
				print(f"one or more of the required files ({host_file_keys}) for host {host} is not specfied: skipping ViFi for host {host}")
				continue
			hosts_to_use.append(host)
									

		for host, virus in itertools.product(hosts_to_use, config[dataset]['analysis_viruses'].keys()):
			condition = f"{dataset}_vifi{i}"
			analysis_conditions.append({
				'experiment' : dataset,
				'host' 			 : host,
				'host_fasta' : config[dataset]['analysis_hosts'][host],
				'host_mappability' : config[dataset]['vifi_params']['host_info'][host]['mappability'],
				'host_mappability_exclude' : config[dataset]['vifi_params']['host_info'][host]['mappability_exclude'],				
				'host_genes' : config[dataset]['vifi_params']['host_info'][host]['genes'],				
				'host_exons' : config[dataset]['vifi_params']['host_info'][host]['exons'],				
				'host_oncogenes' : config[dataset]['vifi_params']['host_info'][host]['oncogenes'],	
				'host_centromeres' : config[dataset]['vifi_params']['host_info'][host]['centromeres'],	
				'host_conserved_regions' : config[dataset]['vifi_params']['host_info'][host]['conserved_regions'],		
				'host_segdup' : config[dataset]['vifi_params']['host_info'][host]['segdup'],																
				'virus'      : virus,		
				'virus_fasta': config[dataset]['analysis_viruses'][virus],	
				'analysis_condition': condition,
				'tool'			 : 'vifi',			
				'score_reads' : 0,
				'score_ints' : score_ints,
				'score_merged_ints' : score_merged_ints,
				'score_ints_window' : score_ints_window,
				'score_ints_tool' : "vifi",
				})
			i += 1

	#### parameters for verse ####
	# paramters should be lists, so that we can do combinations of all
	if 'verse_params' in config[dataset]:
		assert hasattr(config[dataset]['verse_params']['detection_mode'], '__iter__')
		assert hasattr(config[dataset]['verse_params']['flank_region_size'], '__iter__')
		assert hasattr(config[dataset]['verse_params']['sensitivity_level'], '__iter__')
		assert hasattr(config[dataset]['verse_params']['min_contig_length'], '__iter__')
		assert hasattr(config[dataset]['verse_params']['blastn_evalue_thrd'], '__iter__')	
		assert hasattr(config[dataset]['verse_params']['similarity_thrd'], '__iter__')
		assert hasattr(config[dataset]['verse_params']['chop_read_length'], '__iter__')
		assert hasattr(config[dataset]['verse_params']['minIdentity'], '__iter__')
 
	# each combination of these are a unique 'analysis condition' for our pipeline
		i = 0 
	
	 
		for host, virus, detection_mode, flank_region_size, sensitivity_level, min_contig_length, blastn_evalue_thrd, similarity_thrd, chop_read_length, minIdentity in itertools.product(
																		config[dataset]['analysis_hosts'].keys(),
																		config[dataset]['analysis_viruses'].keys(),
																		config[dataset]['verse_params']['detection_mode'], 
																		config[dataset]['verse_params']['flank_region_size'], 
																		config[dataset]['verse_params']['sensitivity_level'],
																		config[dataset]['verse_params']['min_contig_length'],
																		config[dataset]['verse_params']['blastn_evalue_thrd'],
																		config[dataset]['verse_params']['similarity_thrd'],
																		config[dataset]['verse_params']['chop_read_length'],
																		config[dataset]['verse_params']['minIdentity'],
																		):
		
			condition = f"{dataset}_verse{i}"
		
			analysis_conditions.append({
				'experiment' : dataset,
				'host' 			: host,
				'host_fasta': config[dataset]['analysis_hosts'][host],
				'virus'     : virus,
				'virus_fasta': config[dataset]['analysis_viruses'][virus],
				'analysis_condition': condition,
				'detection_mode' : detection_mode,
				'flank_region_size' : flank_region_size,
				'sensitivity_level' : sensitivity_level,
				'min_contig_length' : min_contig_length,
				'blastn_evalue_thrd': blastn_evalue_thrd,
				'similarity_thrd'   : similarity_thrd,
				'chop_read_length'  : chop_read_length,
				'minIdentity'       : minIdentity,
				'score_ints' : score_ints,
				'score_merged_ints' : score_merged_ints,
				'score_ints_window' : score_ints_window,
				'score_ints_tool' : "verse",
				'tool'			 : 'verse',	
			})
			i += 1



##### parameters that are common to each experiment ####
for cond_dict in analysis_conditions:
	dataset = cond_dict['experiment']
	cond_dict['bam_suffix'] = '.sam'
	cond_dict['R1_suffix'] = '1.fq'
	cond_dict['R2_suffix'] = '2.fq'
	cond_dict['read_folder'] = os.path.join(config[dataset]['out_directory'], dataset, "sim_reads")
	cond_dict['outdir'] = config[dataset]['out_directory'] 
	cond_dict['score_ints'] = score_ints
	cond_dict['score_ints_window'] = score_ints_window
	cond_dict['exp'] = cond_dict['experiment']

	
# make data frame 
analysis_df = pd.DataFrame(analysis_conditions, columns = column_names)



## our pipeline also requires a dataframe called 'toDo', containing the analysis paramters for each sample in each dataset.  Build this using sim_df (from simulation) and analysis_df (analysis parameters).  This must have the following columns:

column_names = ['dataset', 'config_dataset', 'experiment', 'sample', 'host', 'host_fasta', 'virus', 'virus_fasta', 'merge', 'trim', 'dedup', 'unique', 'outdir', 'bwa_mem_params', 'R1_file', 'R2_file', 'bam_file', 'adapter_1', 'adapter_2', 'postargs', 'merge_dist', 'merge_n_min', 'clip_cutoff', 'cigar_tol', 'min_mapq']

rows = []

# build rows of toDo
for i, row in analysis_df.iterrows():

	# skip any rows we don't want to analyse with our pipeline
	if row['tool'] != 'pipeline':
		continue

	# get samples for this simulated dataset
	experiment = row['experiment']
	dataset = row['analysis_condition']
	samples = sim_df[sim_df['experiment'] == experiment]['sample']
	
	# make a row for each sample in this dataset
	for sample in samples:
		
		# use information we already worked out from config
		todo_row = dict(row)
		todo_row['dataset'] = dataset
		todo_row['config_dataset'] = experiment
		
		# add sample name
		todo_row['sample'] = sample
		todo_row['unique'] = f"{dataset}+++{sample}"
		
		# read file names
		read_prefix = os.path.join(row['outdir'], experiment, 'sim_reads', sample)
		todo_row['R1_file'] = read_prefix + row['R1_suffix']
		todo_row['R2_file'] = read_prefix + row['R2_suffix']
		todo_row['bam_file'] = read_prefix + row['bam_suffix']
		
		rows.append(todo_row)
		
toDo = pd.DataFrame(rows, columns = column_names)

# make dictionary of reference names and fasta files
ref_names = make_reference_dict(toDo)

# construct arguments for postprocess.R script for each dataset

POSTARGS, TOSORT, SORTED = make_post_args(config)


#####################################################
############ wildcard constraints ###################
#####################################################


wildcard_constraints:
	cond = "|".join(set(sim_df.loc[:, 'condition'])),
	exp = "|".join(set(sim_df.loc[:, 'experiment'])),
	rep = "|".join(set([str(i) for i in sim_df.loc[:, 'replicate']])),
	dset = "|".join(set(analysis_df.loc[:, 'analysis_condition'])),
	samp = "|".join(set(toDo.loc[:, 'sample'])),
	host = "|".join(set(analysis_df.loc[:, 'host'])),
	virus = "|".join(set(analysis_df.loc[:, 'virus'])),
	post = "|\.post",
	merged = "|\.merged",
	analysis_condition = "|".join(analysis_df['analysis_condition']),
	outpath = "|".join(set(analysis_df.loc[:, 'outdir'])),
	
localrules: write_summary, write_analysis_summary#, combine_int_scores, combine_read_scores


#####################################################
################## ruleorder ########################
#####################################################

# to resolve conflicts between polyidus and combine_ints (our analysis pipeline)
# rule polyidus has a wildcard contstraint so it can't be used for our pipeline
# so make it higher in the priority

ruleorder: vifi > polyidus > verse > combine_ints > merged_bed

#####################################################
################### target files ####################
#####################################################

# target files for simulation
sim_targets = sim_df.loc[:,'annotated_info_filename']

# summaries about how experiment was conducted
exp_targets = {}
for i, row in sim_df.iterrows():
	exp_targets[row['experiment']] = row['out_directory']
experiments = list(exp_targets.keys())

sim_summaries = expand("{outpath}/{exp}/simulation_summary.tsv", 
			zip,
			exp = experiments, 
			outpath = [exp_targets[exp] for exp in experiments]
			)


analysis_summaries = expand("{outdir}/{experiment}/analysis_conditions.tsv", 
			zip,
			exp = experiments, 
			outpath = [exp_targets[exp] for exp in experiments]
			)


# outputs from our pipeline
merged_targets = set()
for i, row in analysis_df.iterrows():
	if row['tool'] != 'pipeline':
		continue
		
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['analysis_condition']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
	host = row['host']
	virus = row['virus']
		
	for samp, post in itertools.product(samples, (".post", "")):
		merged_targets.add(
			f"{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.merged.bed"
		)


# polyidus outputs
polyidus_targets = set()
for i, row in analysis_df.iterrows():
	if row['tool'] != 'polyidus':
		continue
		
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['analysis_condition']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
	host = row['host']
	virus = row['virus']
		
	for samp in samples:
		polyidus_targets.add(
			f"{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.txt"
		)
		polyidus_targets.add(
			f"{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}.tsv"
		)


# vifi outputs
vifi_targets = set()
for i, row in analysis_df.iterrows():
	if row['tool'] != 'vifi':
		continue
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['analysis_condition']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
	host = row['host']
	virus = row['virus']
		
	for samp in samples:
		vifi_targets.add(
			f"{outpath}/{dset}/vifi.{samp}.{host}.{virus}/output.clusters.txt"
		)
		vifi_targets.add(
			f"{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}.tsv"
		)
		

# verse targets
verse_targets = set()
for i, row in analysis_df.iterrows():
	if row['tool'] != 'verse':
		continue
	outpath = row['outdir']
	exp = row['experiment']
	dset = row['analysis_condition']
		
	samples = sim_df[sim_df['experiment'] == exp]['sample']
		
	for samp in samples:
		verse_targets.add(
			f"{outpath}/{dset}/{samp}.{host}.{virus}/integration-sites.txt"
		)
	
# summary files

# scored reads
scored_summaries = set()
for i, row in analysis_df.iterrows():
	if row['score_ints'] == 1 or row['score_merged_ints'] == 1:
		scored_summaries.add(f"{row['outdir']}/{row['experiment']}/{row['experiment']}.scored_ints_summary.tsv")
	if 'score_reads' in row:
		if row['score_reads'] == 1:
			scored_summaries.add(f"{row['outdir']}/{row['experiment']}/{row['experiment']}.scored_reads_summary.tsv")

rule all:
	input: 
		set(sim_targets),
		set(sim_summaries),
		set(analysis_summaries),
		merged_targets,
		scored_summaries,
		polyidus_targets,
		vifi_targets,
		verse_targets
		
		
#####################################################
############### simulate integrations ###############
#####################################################


include: "snakemake_rules/simulate_integrations.smk"
include: "snakemake_rules/art.smk"
include: "snakemake_rules/annotate_reads.smk"

#####################################################
################### analyse data ####################
#####################################################

include: "snakemake_rules/preprocessing.smk"
include: "snakemake_rules/alignment.smk"
include: "snakemake_rules/find_ints.smk"
include: "snakemake_rules/postprocessing.smk"


#####################################################
##################### polyidus ######################
#####################################################
def analysis_df_value(wildcards, column_name):
	
	# get a value from the row of the df corresponding to this analysis condition
	unique = f"{wildcards.dset}"

	return analysis_df.loc[(analysis_df['analysis_condition'] == unique).idxmax(), column_name] 
	
def sim_df_value(wildcards, column_name):
	
	
	# get a value from the row of the df corresponding to this analysis condition
	unique = f"{wildcards.exp}__{wildcards.samp}"
	return sim_df.loc[(sim_df['unique'] == unique).idxmax(), column_name] 
	

rule bwt2_index:
	input:
		fasta = lambda wildcards: ref_names[wildcards.genome]
	output:
		multiext("{outpath}/references/{genome}/{genome}", 
							".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2"
						)
	container:
		"docker://szsctt/polyidus:2"
	params:
		prefix = lambda wildcards, output: path.splitext(path.splitext(output[0])[0])[0]
	resources:
		mem_mb= lambda wildcards, attempt, input: int(attempt * 5 * (os.stat(input.fasta).st_size/1e6)),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"bowtie2-build {input} {params.prefix}"

def get_polyidus_reads(wildcards, read_num):
	assert read_num in (1, 2)
	if read_num == 1:
		return f"{wildcards.outpath}/{analysis_df_value(wildcards, 'exp')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, 'R1_suffix')}"
	if read_num == 2:
		return f"{wildcards.outpath}/{analysis_df_value(wildcards, 'exp')}/sim_reads/{wildcards.samp}{analysis_df_value(wildcards, 'R2_suffix')}"	

rule polyidus:
	input:
		fastq1 = lambda wildcards: get_polyidus_reads(wildcards, 1),
		fastq2 = lambda wildcards: get_polyidus_reads(wildcards, 2),
		host_idx =  "{outpath}/references/{host}/{host}.1.bt2",
		virus_idx = "{outpath}/references/{virus}/{virus}.1.bt2"
	output:
		temp_ints = temp("{outpath}/{dset}/polyidus.{host}.{virus}.{samp}/results/exactHpvIntegrations.tsv"),
		ints = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.txt",
		fake_merged = temp("{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.merged.bed")
	params:
		output = lambda wildcards, output: path.dirname(path.dirname(output.temp_ints)),
		host_idx = lambda wildcards, input: path.splitext(path.splitext(input.host_idx)[0])[0],
		virus_idx = lambda wildcards, input: path.splitext(path.splitext(input.virus_idx)[0])[0],
	resources:
		mem_mb= lambda wildcards, attempt: int(attempt * 10000),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1]
	container:
		"docker://szsctt/polyidus:2"
	wildcard_constraints:
		dset = ".+_polyidus\d+"
	shell:
		"""
		rm -rf {params.output}/*
		
		python3 /usr/src/app/src/polyidus.py \
		{params.host_idx} \
		{params.virus_idx} \
		--fastq {input.fastq1} {input.fastq2} \
		--outdir {params.output}
		
		cp {output.temp_ints} {output.ints}
		cp {output.temp_ints} {output.fake_merged}
		"""

#####################################################
####################### vifi ########################
#####################################################

rule host_virus_index:
	input:
		virus = lambda wildcards: ref_names[wildcards.virus],
		host = lambda wildcards: ref_names[wildcards.host]
	output:
		fa = "{outpath}/vifi_refs/data/{virus}/{host}_{virus}.fas",
		idx = multiext("{outpath}/vifi_refs/data/{virus}/{host}_{virus}.fas", ".amb", ".ann", ".bwt", ".pac", ".sa")
	container:
		"docker://namphuon/vifi"
	resources:
		mem_mb= lambda wildcards, attempt, input: int(attempt * 5 * (os.stat(input.host).st_size/1e6 + os.stat(input.virus).st_size/1e6)),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		cat {input.virus} {input.host} > {output.fa}
		bwa index {output.fa}
		"""
		
rule vifi_faidx:
	input:
		fa = lambda wildcards: ref_names[wildcards.host]
	output:
		fai = "{outpath}/vifi_refs/data_repo/{host}/{host}.fa.fai"
	params:
		fai = lambda wildcards, input: input.fa + ".fai"
	container:
		"docker://szsctt/bwa:1"
	shell:
		"""
		samtools faidx {input.fa}
		mv {params.fai} {output.fai}
		"""

def get_vifi_resource(wildcards, resource_name):
	"""Get resources required for vifi"""
	host_idx = analysis_df[(analysis_df['host'] == wildcards.host) & (analysis_df['tool'] == 'vifi')].index[0]
	return analysis_df.loc[host_idx, resource_name]
		
rule vifi_data_repo:
	input:
		fa = lambda wildcards: ref_names[wildcards.host],
		fai = rules.vifi_faidx.output.fai,
		mappability = lambda wildcards: get_vifi_resource(wildcards, 'host_mappability'),
		mappability_exclude = lambda wildcards: get_vifi_resource(wildcards, 'host_mappability_exclude'),
		genes = lambda wildcards: get_vifi_resource(wildcards, 'host_genes'),
		exons = lambda wildcards: get_vifi_resource(wildcards, 'host_exons'),
		oncogenes = lambda wildcards: get_vifi_resource(wildcards, 'host_oncogenes'),
		centromeres = lambda wildcards: get_vifi_resource(wildcards, 'host_centromeres'),
		conserved = lambda wildcards: get_vifi_resource(wildcards, 'host_conserved_regions'),
		segdup = lambda wildcards: get_vifi_resource(wildcards, 'host_segdup')	
	output:
		chromosomes = "{outpath}/vifi_refs/data_repo/{host}/{host}_chromosome-list.txt",
		file_list = "{outpath}/vifi_refs/data_repo/{host}/file_list.txt",
		fa = "{outpath}/vifi_refs/data_repo/{host}/{host}.fa",
		mappability = "{outpath}/vifi_refs/data_repo/{host}/{host}.mappability.bed",
		mappability_exclude = "{outpath}/vifi_refs/data_repo/{host}/{host}.mappability-exclude.bed",
		genes = "{outpath}/vifi_refs/data_repo/{host}/{host}.genes.gff",
		exons = "{outpath}/vifi_refs/data_repo/{host}/{host}.exons.gff"	,
		oncogenes = "{outpath}/vifi_refs/data_repo/{host}/{host}.oncogenes.gff",
		centromeres = "{outpath}/vifi_refs/data_repo/{host}/{host}.centromeres.bed",
		conserved = "{outpath}/vifi_refs/data_repo/{host}/{host}.conserved.bed",
		segdup = "{outpath}/vifi_refs/data_repo/{host}/{host}.segdup.bed"
	shell:
		"""
		cp {input.fa} {output.fa}
		cp {input.mappability} {output.mappability}
		cp {input.mappability_exclude} {output.mappability_exclude}
		cp {input.genes} {output.genes}
		cp {input.exons} {output.exons}
		cp {input.oncogenes} {output.oncogenes}
		cp {input.centromeres} {output.centromeres}
		cp {input.conserved} {output.conserved}
		cp {input.segdup} {output.segdup}
		
		awk 'BEGIN {{ ORS = " " }} {{a[$1]}} END {{for (i in a) print i}}' \
				{input.fai} > {output.chromosomes}
				
		echo "fa_file 		                  $(basename {output.fa})" > {output.file_list}
		echo "chrLen_file 		              $(basename {input.fai})" >> {output.file_list}
  	echo "duke35_filename 		          $(basename {output.mappability})" >> {output.file_list}
  	echo "mapability_exclude_filename   $(basename {output.mappability_exclude})" >> {output.file_list}
  	echo "gene_filename 		            $(basename {output.genes})" >> {output.file_list}
  	echo "exon_file 		                $(basename {output.exons})" >> {output.file_list}
  	echo "oncogene_filename 		        $(basename {output.oncogenes})" >> {output.file_list}
  	echo "centromere_filename 		      $(basename {output.centromeres})" >> {output.file_list}
  	echo "conserved_regions_filename 		$(basename {output.conserved})" >> {output.file_list}
  	echo "segdup_filename 		          $(basename {output.segdup})" >> {output.file_list}		
		"""
	
def vifi_hosts(wildcards):
	"""get all the names of the hosts used for vifi"""
	subset = analysis_df[(analysis_df['tool'] == 'vifi') & (analysis_df['outdir'] == wildcards.outpath)]
	return set(subset['host'])
	
rule data_repo_host_list:
	input: 
		lambda wildcards: [f"{wildcards.outpath}/vifi_refs/data_repo/{host}/file_list.txt" for host in vifi_hosts(wildcards)]
	output: "{outpath}/vifi_refs/data_repo/reference.txt"
	run:
		shell("rm -f {output}")
		shell("touch {output}")	
		hosts = vifi_hosts(wildcards)	
		for host in hosts:
			shell("echo {host} >> {output}")
		
rule vifi:
	input:
		idx = rules.host_virus_index.output.idx,
		fa = rules.host_virus_index.output.fa,
		host_list = rules.data_repo_host_list.output[0],
		fastq1 = lambda wildcards: get_polyidus_reads(wildcards, 1),
		fastq2 = lambda wildcards: get_polyidus_reads(wildcards, 2),
		chrom_list = rules.vifi_data_repo.output.chromosomes,
	output:
		clusters = "{outpath}/{dset}/vifi.{samp}.{host}.{virus}/output.clusters.txt",
		ints = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.txt",
		fake_merged = temp("{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.merged.bed")
	params:
		reference_repo = lambda wildcards, input: os.path.dirname(input.fa),
		aa_data_repo = lambda wildcards, input: os.path.dirname(input.host_list),
		outdir = lambda wildcards, output: os.path.dirname(output.clusters),
		reference = lambda wildcards, input: os.path.splitext(input.idx[0])[0]
	wildcard_constraints:
		dset = ".+_vifi\d+"
	container:
		"docker://szsctt/vifi:1"
	resources:
		mem_mb= lambda wildcards, attempt: int(attempt * 10000),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1]
	threads: 5
	shell:
		"""
		export AA_DATA_REPO=$(realpath {params.aa_data_repo})
		export REFERENCE_REPO=$(realpath {params.reference_repo})
		
		python $VIFI_DIR/scripts/run_vifi.py \
		-c {threads} \
		-f {input.fastq1} -r {input.fastq2} \
		--reference {input.fa} \
		-v {wildcards.virus} \
		-o {params.outdir} \
		-d True \
		-C {input.chrom_list}
		
		cp {output.clusters} {output.ints}
		cp {output.clusters} {output.fake_merged}
		"""			

#####################################################
###################### verse ########################
#####################################################

# verse uses a different version of bowtie2 than polyidus
# in theory they could share the same indexes
# but just to be safe, index again
rule bwt2_verse:
	input:
		fasta = lambda wildcards: ref_names[wildcards.genome]
	output:
		multiext("{outpath}/verse_references/{genome}/{genome}", 
							".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.2.bt2"
						)
	container:
		"docker://szsctt/verse:1"
	params:
		prefix = lambda wildcards, output: path.splitext(path.splitext(output[0])[0])[0]
	resources:
		mem_mb= lambda wildcards, attempt, input: int(attempt * 5 * (os.stat(input.fasta).st_size/1e6)),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"bowtie2-build {input} {params.prefix}"
		
rule blastplus_verse:
	input:
		fasta = lambda wildcards: ref_names[wildcards.genome]
	output:
		multiext("{outpath}/verse_references/{genome}/{genome}", 
							".nhr", ".nin", ".nsq"
						)
	container:
		"docker://szsctt/verse:1"
	params:
		prefix = lambda wildcards, output: path.splitext(output[0])[0]
	resources:
		mem_mb= lambda wildcards, attempt, input: int(attempt * 5 * (os.stat(input.fasta).st_size/1e6)),
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"makeblastdb -in {input} -dbtype nucl -out {params.prefix}"
		
rule verse_config:
	input:
		fastq1 = lambda wildcards: get_polyidus_reads(wildcards, 1),
		fastq2 = lambda wildcards: get_polyidus_reads(wildcards, 2),
		virus_fasta = lambda wildcards: ref_names[wildcards.virus],
		host_fasta = lambda wildcards: ref_names[wildcards.host],
		host_idx =  "{outpath}/verse_references/{host}/{host}.1.bt2",
		virus_blast = "{outpath}/verse_references/{virus}/{virus}.nin",
		host_blast = "{outpath}/verse_references/{host}/{host}.nin"
	output:
		config = "{outpath}/{dset}/{samp}.{host}.{virus}/config.txt"
	params:
		host_bowtie = lambda wildcards, input: path.splitext(path.splitext(path.realpath(input.host_idx))[0])[0],
		virus_blast = lambda wildcards, input: path.splitext(path.realpath(input.virus_blast))[0],
		host_blast = lambda wildcards, input: path.splitext(path.realpath(input.host_blast))[0],
		host_fasta = lambda wildcards, input: path.splitext(path.splitext(path.realpath(input.host_idx))[0])[0] + '.fa',
		virus_fasta = lambda wildcards, input: path.splitext(path.realpath(input.virus_blast))[0] + '.fa',
		detection_mode = lambda wildcards: analysis_df_value(wildcards, 'detection_mode'),
		flank_region_size = lambda wildcards: analysis_df_value(wildcards, 'flank_region_size'),
		sensitivity_level = lambda wildcards: analysis_df_value(wildcards, 'sensitivity_level'),
		min_contig_length = lambda wildcards: analysis_df_value(wildcards, 'min_contig_length'),
		blastn_evalue_thrd = lambda wildcards: analysis_df_value(wildcards, 'blastn_evalue_thrd'),
		similarity_thrd = lambda wildcards: analysis_df_value(wildcards, 'similarity_thrd'),
		chop_read_length = lambda wildcards: analysis_df_value(wildcards, 'chop_read_length'),
		minIdentity = lambda wildcards: analysis_df_value(wildcards, 'minIdentity'),		
	shell:
		"""
  		echo "fastq1 = $(realpath {input.fastq1})" > {output.config}
  		echo "fastq2 = $(realpath {input.fastq2})" >> {output.config}
  		echo 'detect_integration = yes' >> {output.config}
  		echo 'detect_mutation = no' >> {output.config}
  		echo 'thread_no = {threads}' >> {output.config}
  		echo 'blastn_bin = /usr/bin/blastn' >> {output.config}
  		echo 'bowtie_bin = /usr/bin/bowtie2' >> {output.config}
  		echo 'bwa_bin = /usr/bin/bwa' >> {output.config}
  		echo 'trinity_script = /trinityrnaseq_r2013-02-16/Trinity.pl' >> {output.config}
  		echo 'SVDetect_dir = /SVDetect_r0.8' >> {output.config}
  		echo 'virus_database = {params.virus_fasta}' >> {output.config}
  		echo 'bowtie_index_human = {params.host_bowtie}' >> {output.config}
  		echo 'blastn_index_human = {params.host_blast}' >> {output.config}
  		echo 'blastn_index_virus = {params.virus_blast}' >> {output.config}
  		echo 'detection_mode     = {params.detection_mode}' >> {output.config}
			echo 'flank_region_size  = {params.flank_region_size}' >> {output.config}
			echo 'sensitivity_level  = {params.sensitivity_level}' >> {output.config}
  		echo 'min_contig_length = {params.min_contig_length}' >> {output.config}
  		echo 'blastn_evalue_thrd = {params.blastn_evalue_thrd}' >> {output.config}
  		echo 'similarity_thrd = {params.similarity_thrd}' >> {output.config}
  		echo 'chop_read_length = {params.chop_read_length}' >> {output.config}
  		echo 'minIdentity = {params.minIdentity}' >> {output.config}
  		
  		ln -sf $(realpath {input.host_fasta}) {params.host_fasta}
  		ln -sf $(realpath {input.virus_fasta}) {params.virus_fasta}
  		
		"""

rule verse:
	input:
		config = rules.verse_config.output.config,
		fastq1 = lambda wildcards: get_polyidus_reads(wildcards, 1),
		fastq2 = lambda wildcards: get_polyidus_reads(wildcards, 2),
		virus_fasta = lambda wildcards: ref_names[wildcards.virus],
		host_fasta = lambda wildcards: ref_names[wildcards.host],
		host_idx =  "{outpath}/verse_references/{host}/{host}.1.bt2",
		virus_blast = "{outpath}/verse_references/{virus}/{virus}.nin",
		host_blast = "{outpath}/verse_references/{host}/{host}.nin"
	output:
		output = "{outpath}/{dset}/{samp}.{host}.{virus}/integration-sites.txt",
		ints = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.txt",
		fake_merged = temp("{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations.merged.bed")
	params:
		workdir = lambda wildcards, output: path.dirname(path.realpath(output.output)),
		config = lambda wildcards, input: path.basename(input.config),
		currdir = lambda wildcards: os.getcwd()
	wildcard_constraints:
		dset = ".+_verse\d+"
	container:
		"docker://szsctt/verse:1"
	
	shell:
		"""
		cd {params.workdir}
		pwd
		ls
		perl /var/work/VirusFinder2.0/VirusFinder.pl -c {params.config}
		
		cd {params.currdir}
		# virusFinder only makes an output file if it finds integration sites
		# but snakemake always requires the same output files
		if [ ! -e {output.output} ]; then
			touch {output.output}
		fi
		
		cp {output.output} {output.ints}
		cp {output.output} {output.fake_merged}
		"""
		


#####################################################
############ compare sim with analysis ##############
#####################################################


rule write_analysis_summary_all_tools:
	output:
		pipeline_conditions = "{outpath}/{experiment}/pipeline_analysis_conditions.tsv",
		analysis_conditions = "{outpath}/{experiment}/analysis_conditions.tsv"
	run:
		toDo.to_csv(output.pipeline_conditions, sep='\t', index=False)
		analysis_df.to_csv(output.analysis_conditions, sep='\t', index=False)

def get_merged_reads(wildcards):
	# for our pipeline, return result depending on if we merged or not
	if re.search("analysis", wildcards.dset):
		return get_for_align(wildcards, "merged")
	# other pipelines - no merged reads
	else:
		return rules.touch_merged.output.merged
		
rule merged_list:
	input:
		merged = lambda wildcards: get_merged_reads(wildcards)
	output:
		merged_reads = "{outpath}/{dset}/merged_reads/{samp}.merged_reads.txt"
	run:
		if os.path.basename(os.path.dirname(input.merged)) == "combined_reads":
			shell("touch {output.merged_reads}")
		else:
			shell("zcat {input.merged} | awk '{{if (NR%4==1) print substr($0, 2)}}' > {output.merged_reads}")

		
rule score_reads:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		sim_sam = "{outpath}/{exp}/sim_reads/{samp}.sam",
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
		merged_reads = rules.merged_list.output.merged_reads
	output:
		scored_reads = temp("{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}.tsv"),
		scored_reads_uniq = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}.uniq.tsv",
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 50000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	threads: lambda wildcards, attempt: (10, 15, 20, 20)[attempt - 1]
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_reads.py \
		--sim-info {input.sim_info} \
		--sim-sam {input.sim_sam} \
		--analysis-info {input.analysis_info} \
		--merged-reads {input.merged_reads} \
		--output {output.scored_reads} \
		--threads {threads} 
		
		(head -n 1 {output.scored_reads} && tail -n +2 {output.scored_reads} | sort | uniq) > {output.scored_reads_uniq}
		"""
		
rule read_score_summary:
	input:
		scores = rules.score_reads.output.scored_reads_uniq,
		sim_info = rules.annotate_reads.output.annotated_info,
		sim_sam = "{outpath}/{exp}/sim_reads/{samp}.sam",
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
	output:
		summary = "{outpath}/{exp}/scored_reads/{dset}.{samp}.{host}.{virus}{post}_summary.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 50000,
		time = lambda wildcards, attempt: ('24:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""	
		echo -e "sim_info_file\tsim_sam_file\tanalysis_info_file\tresults_file\tjunc_type\tscore_type\ttrue_positives\ttrue_negatives\tfalse_positives\tfalse_negatives" > {output.summary}
		
		TP=$(awk -F "\t" '$4 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$4 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$4 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$4 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\tfound_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$4 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$4 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$4 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$4 ~ /fn/ && /chimeric/' {input.scores} | wc -l)

		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\tfound_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}
		
		TP=$(awk -F "\t" '$5 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$5 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$5 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$5 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\thost_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}	

		TP=$(awk -F "\t" '$5 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$5 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$5 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$5 ~ /fn/ && /chimeric/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\thost_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$6 ~ /tp/ && /discord/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$6 ~ /tn/ && /discord/' {input.scores} | wc -l)
		FP=$(awk -F "\t"  '$6 ~ /fp/ && /discord/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$6 ~ /fn/ && /discord/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tdiscord\tvirus_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		TP=$(awk -F "\t" '$6 ~ /tp/ && /chimeric/' {input.scores} | wc -l)
		TN=$(awk -F "\t" '$6 ~ /tn/ && /chimeric/' {input.scores} | wc -l)
		FP=$(awk -F "\t" '$6 ~ /fp/ && /chimeric/' {input.scores} | wc -l)
		FN=$(awk -F "\t" '$6 ~ /fn/ && /chimeric/' {input.scores} | wc -l)
		
		echo -e "{input.sim_info}\t{input.sim_sam}\t{input.analysis_info}\t{input.scores}\tchimeric\tvirus_score\t${{TP}}\t${{TN}}\t${{FP}}\t${{FN}}" >> {output.summary}

		"""

rule combine_read_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "reads")
	output:
		"{outpath}/{exp}/{exp}.scored_reads_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""

rule score_integrations:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.txt",
		merged_reads = rules.merged_list.output.merged_reads
	output:
		summary = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}_summary.tsv",
		temp = temp("{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.tmp.tsv"),
		scored_ints = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	params:
		tool = lambda wildcards: f"--analysis-tool {analysis_df_value(wildcards, 'score_ints_tool')}",
		window = lambda wildcards: f"--window {analysis_df_value(wildcards, 'score_ints_window')}",
	threads: 1
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_integrations.py \
		--sim-info {input.sim_info} \
		--found-info {input.analysis_info} \
		--output {output.temp} \
		--summary {output.summary} \
		{params.tool} {params.window}
		
		python3 scripts/int_fp_type.py \
		--sim-info {input.sim_info} \
		--merged {input.merged_reads} \
		--scored-ints {output.temp} \
		--output {output.scored_ints}
		"""
	
rule score_merged_integrations:
	input:
		sim_info = rules.annotate_reads.output.annotated_info,
		analysis_info = "{outpath}/{dset}/ints/{samp}.{host}.{virus}.integrations{post}.merged.bed"
	output:
		summary = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.merged_summary.tsv",
		scored_ints = "{outpath}/{exp}/scored_ints/{dset}.{samp}.{host}.{virus}{post}.merged.tsv"
	resources:
		mem_mb= lambda wildcards, attempt: attempt * 5000,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	params:
		tool = lambda wildcards: f"--analysis-tool {analysis_df_value(wildcards, 'score_ints_tool')}",
		window = lambda wildcards: f"--window {analysis_df_value(wildcards, 'score_ints_window')}",
		merged = "--merged"
	threads: 1
	container:
		"docker://szsctt/simvi:2"
	conda:
		"envs/simvi.yml"
	shell:
		"""
		python3 scripts/score_integrations.py \
		--sim-info {input.sim_info} \
		--found-info {input.analysis_info} \
		--output {output.scored_ints} \
		--summary {output.summary} \
		{params}
		"""
		
rule combine_int_scores:
	input:
		lambda wildcards: scores_to_combine(wildcards, "ints")
	output:
		"{outpath}/{exp}/{exp}.scored_ints_summary.tsv"
	container:
		"docker://ubuntu:18.04"	
	resources:
		mem_mb= lambda wildcards, attempt, input: attempt * len(input) * 10,
		time = lambda wildcards, attempt: ('2:00:00', '24:00:00', '24:00:00', '7-00:00:00')[attempt - 1],
		nodes = 1
	shell:
		"""
		awk 'FNR==1 && NR!=1 {{ getline; }}; 1 {{print}}' {input} > {output}
		"""
		
# target files for per-read or per-integration comparison of analysis with simulation
def scores_to_combine(wildcards, score_type):

	assert score_type in ("ints", "reads")
	
	if score_type == "ints":
		folder = "scored_ints"
	else:
		folder = "scored_reads"
	
	scored_files = []
	for i, row in analysis_df.iterrows():
	
		# check if we want to score reads or ints for this row
		if score_type == 'reads':
			if 'score_reads' not in row:
				continue
			if row['score_reads'] != 1:
				continue
		if score_type == 'ints':
			if 'score_ints' not in row and 'score_merged_ints' not in row:
				continue
			if 'score_ints' in row and 'score_merged_ints' in row:
				if row['score_ints'] != 1 and row['score_merged_ints'] != 1:
					continue
			if 'score_ints' in row and 'score_merged_ints' not in row:
				if row['score_ints'] != 1:
					continue
			if 'score_merged_ints' in row and 'score_ints' not in row:
				if row['score_merged_ints'] != 1:
					continue
					
		# only scoring reads for our pipeline
		if score_type == 'reads' and not re.search("analysis", analysis_condition):
			continue
		
		# get information for this row
		exp = row['experiment']

		if exp !=  wildcards.exp:
			continue
		outpath = row['outdir']
		analysis_condition = row['analysis_condition']
		host = row['host']
		virus = row['virus']
	
		# get samples for this experiment
		samples = set(sim_df[sim_df['experiment'] == exp]['sample'])
		
		# we only have merged integrations for integration scoring
		# only do this if the user requested merged integrations to be scored
		merged = ['']
		if score_type == 'ints' and 'score_merged_ints' in row:
			if row['score_merged_ints'] == 1:
				merged = ['.merged', '']
		
		# we only have postprocessed data for our pipeline
		if re.search("analysis", analysis_condition):
			post = ['.post', '']
		else:
			post = ['']
			
		scored_files += [
		f"{outpath}/{exp}/{folder}/{analysis_condition}.{samp}.{host}.{virus}{post}{merged}_summary.tsv"
			for samp, post, merged
			in itertools.product(samples, post, merged)
		]
	
	return scored_files

